{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table.png](pics/table.png)\n",
    "\n",
    "![dt_decisiveness.png](pics/dt_decisiveness.png)\n",
    "\n",
    "![table_to_tree.png](pics/table_to_tree.png)\n",
    "\n",
    "![best_feature.png](pics/best_feature.png)\n",
    "\n",
    "\n",
    "![first_split](pics/first_split.png)\n",
    "\n",
    "![final_grades_tree.png](pics/final_grades_tree.png)\n",
    "\n",
    "## Binary entropy\n",
    "\n",
    "![entropy_explained.png](pics/entropy_explained.png)\n",
    "\n",
    "![prob_winning.png](pics/prob_winning.png)\n",
    "\n",
    "![entropy_formula.png](pics/entropy_formula.png)\n",
    "\n",
    "![entropy_example.png](pics/entropy_example.png)\n",
    "\n",
    "### Quiz\n",
    "\n",
    "What is the entropy for a bucket with a ratio of four red balls to ten blue balls? Input your answer to at least three decimal places.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863120568566631\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "entropy = -(4/14) * log2(4/14) -(10/14) * log2(10/14)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Entropy\n",
    "Last time, you saw this equation for entropy for a bucket with $m$ red balls and $n$ blue balls:\n",
    "\n",
    "\n",
    "We can state this in terms of probabilities instead for the number of red balls as $p_1$  and the number of blue balls as $p_2$:\n",
    "\n",
    "$$entropy = -p_1\\log_2(p_1)-p_2\\log_2(p_2)$$\n",
    "\n",
    "This entropy equation can be extended to the multi-class case, where we have three or more possible values:\n",
    "\n",
    "$$entropy = -p_1\\log_2(p_1) - p_2\\log_2(p_2) - ... - p_n\\log_2(p_n) = -\\sum\\limits_{i=1}^n p_i\\log_2(p_i)$$\n",
    "\n",
    "The minimum value is still 0, when all elements are of the same value. The maximum value is still achieved when the outcome probabilities are the same, but the upper limit increases with the number of different outcomes. (For example, you can verify the maximum entropy is 2 if there are four different possibilities, each with probability 0.25.)\n",
    "\n",
    "### Quiz\n",
    "\n",
    "If we have a bucket with eight red balls, three blue balls, and two yellow balls, what is the entropy of the set of balls? Input your answer to at least three decimal places.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3346791410515946\n"
     ]
    }
   ],
   "source": [
    "entropy = -(8/13) * log2(8/13) -(3/13) * log2(3/13) - (2/13) * log2(2/13)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz Information Gain\n",
    "\n",
    "![quiz_information_gain.png](pics/quiz_information_gain.png)\n",
    "\n",
    "Where did we gain more information? Where did we gain less? Match the columns.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "Tree 1: Small, Tree 2: Medium, Tree3: Large\n",
    "\n",
    "![information_gain_example.png](pics/information_gain_example.png)\n",
    "\n",
    "![information_gain_calculation.png](pics/information_gain_calculation.png)\n",
    "\n",
    "![information_gain_calculation_example.png](pics/information_gain_calculation_example.png)\n",
    "\n",
    "![information_gain_calculation_example2.png](pics/information_gain_calculation_example2.png)\n",
    "\n",
    "### Maximizing Information Gain\n",
    "\n",
    "![max_ig_1.png](pics/max_ig_1.png)\n",
    "\n",
    "![max_ig_2.png](pics/max_ig_2.png)\n",
    "\n",
    "![max_ig_3.png](pics/max_ig_3.png)\n",
    "\n",
    "![max_ig_4.png](pics/max_ig_4.png)\n",
    "\n",
    "![max_ig_5.png](pics/max_ig_5.png)\n",
    "\n",
    "\n",
    "For the following quiz, consider the data found in `ml_bugs.csv`, consisting of twenty-four made-up insects measured on their length and color.\n",
    "\n",
    "Which of the following splitting criteria provides the most information gain for discriminating Mobugs from Lobugs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11260735516748954\n"
     ]
    }
   ],
   "source": [
    "# Here was how I calculated the solution:\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def two_group_ent(first, tot):                        \n",
    "    return -(first/tot*np.log2(first/tot) +           \n",
    "             (tot-first)/tot*np.log2((tot-first)/tot))\n",
    "\n",
    "tot_ent = two_group_ent(10, 24)                       \n",
    "g17_ent = 15/24 * two_group_ent(11,15) + 9/24 * two_group_ent(6,9)                  \n",
    "\n",
    "answer = tot_ent - g17_ent    \n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Decision Trees\n",
    "\n",
    "In order to create decision trees that will generalize to new problems well, we can tune a number of different aspects about the trees. We call the different aspects of a decision tree \"hyperparameters\". These are some of the most important hyperparameters used in decision trees:\n",
    "\n",
    "### Maximum Depth\n",
    "The maximum depth of a decision tree is simply the largest possible length between the root to a leaf. A tree of maximum length $k$ can have at most $2^k$ leaves.\n",
    "\n",
    "![depth_tree.png](pics/depth_tree.png)\n",
    "\n",
    "### Minimum number of samples to split\n",
    "\n",
    "A node must have at least `min_samples_split` samples in order to be large enough to split. If a node has fewer samples than `min_samples_split` samples, it will not be split, and the splitting process stops.\n",
    "\n",
    "![min-samples-split.png](pics/min-samples-split.png)\n",
    "\n",
    "However, `min_samples_split` doesn't control the minimum size of leaves. As you can see in the example on the right, above, the parent node had 20 samples, greater than `min_samples_split = 11`, so the node was split. But when the node was split, a child node was created with that had 5 samples, less than `min_samples_split = 11`.\n",
    "\n",
    "### Minimum number of samples per leaf\n",
    "\n",
    "When splitting a node, one could run into the problem of having 99 samples in one of them, and 1 on the other. This will not take us too far in our process, and would be a waste of resources and time. If we want to avoid this, we can set a minimum for the number of samples we allow on each leaf.\n",
    "\n",
    "![min_sample_per_leaf.png](pics/min_sample_per_leaf.png)\n",
    "\n",
    "This number can be specified as an integer or as a float. If it's an **integer**, it's the minimum **number of samples** allowed in a leaf. If it's a **float**, it's the minimum **percentage** of samples allowed in a leaf. For example, 0.1, or 10%, implies that a particular split will not be allowed if one of the leaves that results contains less than 10% of the samples in the dataset.\n",
    "\n",
    "If a threshold on a feature results in a leaf that has fewer samples than `min_samples_leaf`, the algorithm will not allow that split, but it may perform a split on the same feature at a different threshold, that does satisfy `min_samples_leaf`.\n",
    "\n",
    "###  Quiz\n",
    "\n",
    "Let's test your intuition. Which sizes of features are associated with underfitting and which with overfitting? \n",
    "\n",
    "* Large depth very often causes overfitting, since a tree that is too deep, can memorize the data. \n",
    "* Small depth can result in a very simple model, which may cause underfitting.\n",
    "* Small minimum samples per split may result in a complicated, highly branched tree, which can mean the model has memorized the data, or in other words, overfit. \n",
    "* Large minimum samples may result in the tree not having enough flexibility to get built, and may result in underfitting.\n",
    "\n",
    "## Decision Trees in sklearn\n",
    "\n",
    "In this section, you'll use decision trees to fit a given sample dataset.\n",
    "\n",
    "Before you do that, let's go over the tools required to build this model.\n",
    "\n",
    "For your decision tree model, you'll be using scikit-learn's Decision Tree Classifier class. This class provides the functions to define and fit the model to your data.\n",
    "\n",
    "```\n",
    ">>> from sklearn.tree import DecisionTreeClassifier\n",
    ">>> model = DecisionTreeClassifier()\n",
    ">>> model.fit(x_values, y_values)\n",
    "```\n",
    "\n",
    "In the example above, the model variable is a decision tree model that has been fitted to the data `x_values` and `y_values`. Fitting the model means finding the best tree that fits the training data. Let's make two predictions using the model's `predict()` function.\n",
    "\n",
    "```\n",
    ">>> print(model.predict([ [0.2, 0.8], [0.5, 0.4] ]))\n",
    "[[ 0., 1.]]\n",
    "```\n",
    "\n",
    "The model returned an array of predictions, one prediction for each input array. The first input, `[0.2, 0.8]`, got a prediction of `0.`. The second input, `[0.5, 0.4]`, got a prediction of `1.`.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "When we define the model, we can specify the hyperparameters. In practice, the most common ones are\n",
    "\n",
    "`max_depth`: The maximum number of levels in the tree.\n",
    "`min_samples_leaf`: The minimum number of samples allowed in a leaf.\n",
    "`min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "\n",
    "For example, here we define a model where the maximum depth of the trees `max_depth` is 7, and the minimum number of elements in each leaf `min_samples_leaf` is 10.\n",
    "\n",
    "`>>> model = DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)`\n",
    "\n",
    "### Decision Tree Quiz\n",
    "\n",
    "In this quiz, you'll be given the following sample dataset, and your goal is to define a model that gives 100% accuracy on it.\n",
    "\n",
    "![tree_programming_quiz.png](pics/tree_programming_quiz.png)\n",
    "\n",
    "The data file can be found under the \"data_tree.csv\" tab in the quiz below. It includes three columns, the first 2 comprising of the coordinates of the points, and the third one of the label.\n",
    "\n",
    "The data will be loaded for you, and split into features X and labels y.\n",
    "\n",
    "You'll need to complete each of the following steps:\n",
    "\n",
    "1. Build a decision tree model\n",
    "\n",
    "Create a decision tree classification model using scikit-learn's DecisionTreeClassifier and assign it to the `variablemodel`.\n",
    "\n",
    "2. Fit the model to the data\n",
    "\n",
    "You won't need to specify any of the hyperparameters, since the default ones will yield a model that perfectly classifies the training data. However, we encourage you to play with hyperparameters such as `max_depth` and min_samples_leaf to try to find the simplest possible model.\n",
    "\n",
    "3. Predict using the model\n",
    "\n",
    "Predict the labels for the training set, and assign this list to the variable `y_pred`.\n",
    "4. Calculate the accuracy of the model\n",
    "\n",
    "For this, use the function sklearn function `accuracy_score`. A model's accuracy is the fraction of all data points that it correctly classified.\n",
    "When you hit Test Run, you'll be able to see the boundary region of your model, which will help you tune the correct parameters, in case you need them.\n",
    "\n",
    "Note: This quiz requires you to find an accuracy of 100% on the training set. This is like memorizing the training data! A model designed to have 100% accuracy on training data is unlikely to generalize well to new data. If you pick very large values for your parameters, the model will fit the training set very well, but may not generalize well. Try to find the smallest possible parameters that do the job—then the model will be more likely to generalize well. (This aspect of the exercise won't be graded.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data/data_tree.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the decision tree model and assign it to the variable model.\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model.\n",
    "model.fit(X,y)\n",
    "\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y, y_pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Titanic Survival Exploration with Decision Trees\n",
    "\n",
    "### Getting Started\n",
    "In this lab, you will see how decision trees work by implementing a decision tree in sklearn.\n",
    "\n",
    "We'll start by loading the dataset and displaying some of its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Set a random seed\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "in_file = 'data/titanic_data.csv'\n",
    "full_data = pd.read_csv(in_file)\n",
    "\n",
    "# Print the first few entries of the RMS Titanic data\n",
    "display(full_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that these are the various features present for each passenger on the ship:\n",
    "- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n",
    "- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n",
    "- **Name**: Name of passenger\n",
    "- **Sex**: Sex of the passenger\n",
    "- **Age**: Age of the passenger (Some entries contain `NaN`)\n",
    "- **SibSp**: Number of siblings and spouses of the passenger aboard\n",
    "- **Parch**: Number of parents and children of the passenger aboard\n",
    "- **Ticket**: Ticket number of the passenger\n",
    "- **Fare**: Fare paid by the passenger\n",
    "- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n",
    "- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "Since we're interested in the outcome of survival for each passenger or crew member, we can remove the **Survived** feature from this dataset and store it as its own separate variable `outcomes`. We will use these outcomes as our prediction targets.  \n",
    "Run the code cell below to remove **Survived** as a feature of the dataset and store it in `outcomes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Store the 'Survived' feature in a new variable and remove it from the dataset\n",
    "outcomes = full_data['Survived']\n",
    "features_raw = full_data.drop('Survived', axis = 1)\n",
    "\n",
    "# Show the new dataset with 'Survived' removed\n",
    "display(features_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very same sample of the RMS Titanic data now shows the **Survived** feature removed from the DataFrame. Note that `data` (the passenger data) and `outcomes` (the outcomes of survival) are now *paired*. That means for any passenger `data.loc[i]`, they have the survival outcome `outcomes[i]`.\n",
    "\n",
    "## Preprocessing the data\n",
    "\n",
    "Now, let's do some data preprocessing. First, we'll remove the names of the passengers, and then one-hot encode the features.\n",
    "\n",
    "One-Hot encoding is useful for changing over categorical data into numerical data, with each different option within a category changed into either a 0 or 1 in a separate *new* category as to whether it is that option or not (e.g. Queenstown port or not Queenstown port). Check out [this article](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) before continuing. \n",
    "\n",
    "**Question:** Why would it be a terrible idea to one-hot encode the data without removing the names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the names\n",
    "features_no_names = features_raw.drop(['Name'], axis=1)\n",
    "\n",
    "# One-hot encoding\n",
    "features = pd.get_dummies(features_no_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll fill in any blanks with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Ticket_110152</th>\n",
       "      <th>Ticket_110413</th>\n",
       "      <th>...</th>\n",
       "      <th>Cabin_F G73</th>\n",
       "      <th>Cabin_F2</th>\n",
       "      <th>Cabin_F33</th>\n",
       "      <th>Cabin_F38</th>\n",
       "      <th>Cabin_F4</th>\n",
       "      <th>Cabin_G6</th>\n",
       "      <th>Cabin_T</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 839 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass   Age  SibSp  Parch     Fare  Sex_female  Sex_male  \\\n",
       "0            1       3  22.0      1      0   7.2500           0         1   \n",
       "1            2       1  38.0      1      0  71.2833           1         0   \n",
       "2            3       3  26.0      0      0   7.9250           1         0   \n",
       "3            4       1  35.0      1      0  53.1000           1         0   \n",
       "4            5       3  35.0      0      0   8.0500           0         1   \n",
       "\n",
       "   Ticket_110152  Ticket_110413  ...  Cabin_F G73  Cabin_F2  Cabin_F33  \\\n",
       "0              0              0  ...            0         0          0   \n",
       "1              0              0  ...            0         0          0   \n",
       "2              0              0  ...            0         0          0   \n",
       "3              0              0  ...            0         0          0   \n",
       "4              0              0  ...            0         0          0   \n",
       "\n",
       "   Cabin_F38  Cabin_F4  Cabin_G6  Cabin_T  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0          0         0         0        0           0           0           1  \n",
       "1          0         0         0        0           1           0           0  \n",
       "2          0         0         0        0           0           0           1  \n",
       "3          0         0         0        0           0           0           1  \n",
       "4          0         0         0        0           0           0           1  \n",
       "\n",
       "[5 rows x 839 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = features.fillna(0.0)\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now we're ready to train a model in sklearn. First, let's split the data into training and testing sets. Then we'll train the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(max_depth=6, min_samples_leaf=6, min_samples_split=10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.8707865168539326\n",
      "The test accuracy is 0.8547486033519553\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outro \n",
    "\n",
    "Congratulations! In this section you've learned all about decision trees, and how to use them to make predictions. In the next section, we are going to learn more about some of the concepts we alluded to in this section—how to test and evaluate your model to see how it's performing. See you there!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
