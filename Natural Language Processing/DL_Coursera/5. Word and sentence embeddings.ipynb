{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Distributional semantics: bee and honey vs. bee an bumblebee\n",
    "\n",
    "Why do we need this? Well for example, we need this in search. So let's say we want to do some ranking. For example, we have some keywords, and then we have some candidates to rank. And then we can just compute these kind of similarities between our query and our candidates, and then get the top most similar results.\n",
    "\n",
    "And actually there are numerous of applications of these techniques. For example, you can also think about some ontology learning. What it means is that sometimes you need to represent the hierarchical structure of some area. You need to know that there are some concepts, and there are some examples of these concepts. For example, you might want to know that, I don't know, there are plumbers, and that they can fix tap or faucet. And you need to know that tap and faucet are similar words that present the same concept. This can be also done by distributional semantics, and this is what we are going to cover right now. \n",
    "\n",
    "![ws_1.png](pics/ws_1.png)\n",
    "![ws_1.png](pics/ws_2.png)\n",
    "![ws_1.png](pics/ws_3.png)\n",
    "\n",
    "This is the measure that is usually used, and the idea that goes for all these measures actually would be just distributional hypothesis. It says that you can know the word by the company it keeps. So the meaning of the word is somehow defined by the context of this word.\n",
    "\n",
    "\n",
    "Well if you want to measure a cosine similarity between these long, sparse vectors, maybe it's not a good idea. So it is long, it is noisy, it is too sparse. Let us try to do some dimensionality reduction.\n",
    "\n",
    "![ws_1.png](pics/ws_4.png)\n",
    "\n",
    "You have lots of different options how to do this factorization, and we will get into them later. What we need to know now is that we are going to compare now the rows of v matrix instead of the original sparse rows of X matrix.\n",
    "\n",
    "This way we will get some measure of whether the words are similar, and this will be the output of our model. So far we have looked into how our words occur with other words from a sliding window. So we had some contexts, which would be words from a sliding window. \n",
    "\n",
    "![ws_1.png](pics/ws_5.png)\n",
    "\n",
    "However, we can have some more complicated notion of the contexts. For example, you can have some syntax parses. Then you will know that you have some syntactic dependencies between the words. And you can see that some word has co-occurred with another word and had this type of relationship between them, right? So for example, Australian has co-occurred with scientist as a modifier. So in this model, you will say that your contexts are word plus the type of the relationship. And in this case you will have not a square matrix, but some matrix of words by contexts. And for the contexts you will have the vocabulary of those word class modifier units, okay? This will be actually a better idea to do, because syntax can really help you to understand what is important to local context and what is not. What is just some random co-occurrences that are near, but that are not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explicit and implicit matrix factorization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
