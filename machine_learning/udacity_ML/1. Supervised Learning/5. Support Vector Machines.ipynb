{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM_better_line.png](pics/SVM_better_line.png)\n",
    "\n",
    "### QUIZ QUESTION\n",
    "Which one is a better line?\n",
    "\n",
    "1. The one in the left (yellow) -  This one\n",
    "\n",
    "1. The one in the right (Green)\n",
    "\n",
    "\n",
    "![SVM_max_margin.png](pics/SVM_max_margin.png)\n",
    "\n",
    "## Perceptron error function\n",
    "\n",
    "\n",
    "![SVM_error.png](pics/SVM_error.png)\n",
    "\n",
    "![svm_error_function.png](pics/svm_error_function.png)\n",
    "\n",
    "![svm_error_function_2.png](pics/svm_error_function_2.png)\n",
    "\n",
    "## SVM error function\n",
    "\n",
    "![svm_cl_error_1.png](pics/svm_cl_error_1.png)\n",
    "\n",
    "Remember that we don't want anything between those lines. \n",
    "\n",
    "![svm_cl_error.png](pics/svm_cl_error.png)\n",
    "\n",
    "Now the error starts from the borders because we don't want data within the margins. \n",
    "\n",
    "![svm_error_3.png](pics/svm_error_3.png)\n",
    "\n",
    "So the error starts with 0 at the lower bording line and goes all the way up. In a nutshell, this is the error in support vector machines. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Error \n",
    "\n",
    "![min_margin_error_1.png](pics/min_margin_error_1.png)\n",
    "\n",
    "The goal is to has as large margin as possible, how do we get that? It is actually pretty simple. \n",
    "\n",
    "![min_margin_error_2.png](pics/min_margin_error_2.png)\n",
    "\n",
    "Here is an example to make it more clear: \n",
    "\n",
    "![margin_error_example_1.png](pics/margin_error_example_1.png)\n",
    "\n",
    "## Margin Error Calculation\n",
    "In this section, we'll calculate the distance between the two margins in the SVM.\n",
    "\n",
    "First, recall the notation, where Where $W = (w_1, w_2)$ and $x = (x_1,x_2)$, and $Wx = w_1x_1 + w_2x_2$. \n",
    "\n",
    "Notice that all we have three lines, of the following equations:\n",
    "\n",
    "$$Wx+b=1Wx+b=1$$\n",
    "$$Wx+b=0Wx+b=0$$\n",
    "$$Wx+b=-1Wx+b=−1$$\n",
    "\n",
    "And in order to find the distance between the first and the third, we only need to find the distance between the first two, and multiply by two, as these are three equidistant parallel lines. That is, we need to find the distance between the two lines in Figure 1.\n",
    "\n",
    "![margin-geometry-images.001.jpeg](pics/margin-geometry-images.001.jpeg)\n",
    "\n",
    "Now, notice that since we're only measuring distances between lines, we might as well translate these two lines, so that one of them touches the origin (Figure 2). Thus, our new equations are:\n",
    "\n",
    "$$Wx=0$$\n",
    "$$Wx=1$$\n",
    "\n",
    "![margin-geometry-images.002.jpeg](pics/margin-geometry-images.002.jpeg)\n",
    "\n",
    "Now, the first line has equation $Wx=0$, which means it's orthogonal (perpendicular) to the vector $W = (w_1, w_2)$ colored in red (Figure 3).\n",
    "\n",
    "![margin-geometry-images.003.jpeg](pics/margin-geometry-images.003.jpeg)\n",
    "\n",
    "This vector intersects the line of equation $Wx=1$ at the blue point (Figure 4). Let's say the point has coordinates $(p,q)$. Then, we know two things:\n",
    "\n",
    "* $w1p+w2q=1$ (since the point is over the line), and\n",
    "* $(p,q)$ is a multiple of $(w_1, w_2)$, since the point is over the vector $W = (w_1, w_2)$.\n",
    "\n",
    "We can solve these two equations as follows: Let $(p,q) = k(w_1, w_2)$ for some $k$. That turns our first equation into $k(w_1^2 + w_2^2) = 1$. Therefore, $k = \\frac{1}{w_1^2+w_2^2} = \\frac{1}{|W|^2}$. That means, our blue point represents the vector $\\frac{W}{|W|^2}$, as shown in Figure 4.\n",
    "\n",
    "![margin-geometry-images.004.jpeg](pics/margin-geometry-images.004.jpeg)\n",
    "\n",
    "Now, the distance between the two lines, is simply the norm of the blue vector. Since the denominator is a scalar, one can see that the norm of the vector $\\frac{W}{|W|^2}$ is precisely $\\frac{|W|}{|W|^2} $, which is the same as $\\frac{1}{|W|}$ (Figure 5).\n",
    "\n",
    "![margin-geometry-images.005.jpeg](pics/margin-geometry-images.005.jpeg) \n",
    "\n",
    "And finally, we remember that the desired distance was the sum of these two distances between the consecutive parallel lines (Figure 6). Since each one of them is $\\frac{1}{|W|}$, then the total distance is $\\frac{2}{|W|}$.\n",
    "\n",
    "![margin-geometry-images.008.jpeg](pics/margin-geometry-images.008.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "![error_gd_1.png](pics/error_gd_1.png)\n",
    "\n",
    "## C Parameter\n",
    "\n",
    "C parameter gives us the flexibility to select between prioritizing having a large margin or maximizing the correctly classified points. (it depends on the use case) Also C parameter is a hyperparameter, so it may be needed to use a grid search in order to optimize it.  \n",
    "\n",
    "![c_param_1.png](pics/c_param_1.png)\n",
    "\n",
    "![c_param_2.png](pics/c_param_2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "Here we will explore the polynomial kernel where is needed when a straight line is not enough to separate the points. \n",
    "\n",
    "![pol_k_1.png](pics/pol_k_1.png) \n",
    "\n",
    "In the example above, we can see that a more complex model is needed. \n",
    "\n",
    "Let's change the one dimensional line to a two dimensional plane. \n",
    "\n",
    "![pol_k_2.png](pics/pol_k_2.png)\n",
    "\n",
    "![pol_k_3.png](pics/pol_k_3.png)\n",
    "\n",
    "\n",
    "### Kernel Trick \n",
    "\n",
    "There are times that a line is still not enough to separate the points. Luckily there are two ways to deal with it. The first one is the circular boundary where a circle separates the points \n",
    "\n",
    "![k_trick_c.png](pics/k_trick_c.png)\n",
    "\n",
    "The other one is to think outside the plane, to think in higher dimensions. We introduce the third (z) axis. Now we can think we are living in a building and we are going to send the blue points to say, the second floor of the building and the red points to say the 18th floor of the building and we will put a boundary at the 10th floor and that is how we separate the points. \n",
    "\n",
    "![h_dim_trick.png](pics/h_dim_trick.png)\n",
    "\n",
    "\n",
    "So basically we have two methods. The **circle** method which sacrifices the linearity, and instead of using a linear equation \n",
    "it uses a higher degree polynomial equation. And the **building** method which sacrifices the dimensionality of the data. \n",
    "\n",
    "![kernel_methods.png](pics/kernel_methods.png)\n",
    "\n",
    "Which do you prefer the most? They are actually the same method named the **kernel method**. \n",
    "\n",
    "![kernel_trick_question.png](pics/kernel_trick_question.png)\n",
    "\n",
    "\n",
    "Solution: $x^2 + y^2$\n",
    "\n",
    "![solution_k_trick.png](pics/solution_k_trick.png)\n",
    "\n",
    "It is clear that this is the circle method\n",
    "\n",
    "![k_t_c.png](pics/k_t_c.png)\n",
    "\n",
    "But we said it is the same like the building method. Let's see how. \n",
    "\n",
    "![k_t_b.png](pics/k_t_b.png)\n",
    "\n",
    "Essentially the polynomial kernel trick is that we add dimensions, so our data live in higher dimensions ($n$) and then we return them into the previous number of dimensions ($m$) and we get a polynomial of degree $n-m$ which nicely separates out data. This is named the **polynomial kernel**\n",
    "\n",
    "![summary_1.png](pics/summary_1.png)\n",
    "\n",
    "![pol_kernel_out.png](pics/pol_kernel_out.png)\n",
    "\n",
    "It can happen that we have even more complicated patterns, the we can use a higher order polynomial\n",
    "\n",
    "![high_degree_pol.png](pics/high_degree_pol.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radio based function (RBF) Kernel\n",
    "\n",
    "![rbf_kernel_1.png](pics/rbf_kernel_1.png)\n",
    "\n",
    "Now that mountain range was a bit suspicious, a little too convinient. The question is how do we build a mountain range that will tend to locate the red points in highlands and the blue points in lowlands? Well, here is a technique: \n",
    "\n",
    "![rbg_kernel_2.png](pics/rbg_kernel_2.png)\n",
    "\n",
    "We build a mountain on top of every point, the technical term for this is ´radial basis function´, and we will see the formulas later. So, we built some functions that looked like this, and now we consider combining these functions. How would we do it to build a range that helps us separate the blue and the red points?\n",
    "\n",
    "Here is an idea. Let's flip the middle one (or multiply it by -1)\n",
    "\n",
    "![rbf_kernel_3.png](pics/rbf_kernel_3.png)\n",
    "\n",
    ", and now add the three of them, and by adding them I literally mean the way you add functions, so at every point just add the three heights. If we add them, we get the following function:\n",
    "\n",
    "![rbf_kernel_4.png](pics/rbf_kernel_4.png)\n",
    "\n",
    "and then we can easily draw a line and split the points.\n",
    "\n",
    "![rbf_kernel_5.png](pics/rbf_kernel_5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can multiply it with other weights as well, as can be seen below \n",
    "\n",
    "![rbf_kernel_6](pics/rbf_kernel_6.png)\n",
    "\n",
    "But how do we define these weights? \n",
    "\n",
    "Here is the way to do it. Here are three funnctions\n",
    "\n",
    "![rbf_kernel_7](pics/rbf_kernel_7.png)\n",
    "\n",
    "Now let's record the value of the function at every point, so literally how tall is that point? \n",
    "![rbf_kernel_8](pics/rbf_kernel_8.png)\n",
    "\n",
    "For the first point we have a height $1$ since we constructed the mountain to peak at that point. The other two heights are small since the yellow and the brown mountains are pretty low at this point. So let's say it is $0.08$ and $0.02$. Then we do the same thing for the second point and the third point. Notice that each point will have one value of `1` on their vector of heights since the height of the mountain corresponding to that point is one by construction. In general the other values will be small but we may have some close by points that will give us high values.  But it doesn't matter. \n",
    "\n",
    "The question is how do we find the right linear combination of green, yellow, and brown functions which will be able to separate the blue and the red points? \n",
    "\n",
    "Here is where the magic happens. Let's take these three height vectors and just plot them in a three dimendional space. \n",
    "\n",
    "![rbf_kernel_9](pics/rbf_kernel_9.png)\n",
    "\n",
    "\n",
    "Notice that they are very close to being the three basis vectors, since they have a one in each of the coordinates and almost zero on the other two. As I stated before this is not always the case, but it doesn't matter as long as it gives us a set of points that we can separate. And since we have as many dimensions as points, then this space is very high dimensional, so chances are we'll be able to separate our points well. So with luch in the application of our known SVM algorithm, let's say we are able to separate these red and blue points with this plane over here. \n",
    "\n",
    "![rbf_kernel_10](pics/rbf_kernel_10.png)\n",
    "\n",
    "A plane with equation say $2x - 4y + z = 1$. Let's remember this equation. The reason it is useful is the following. If we take the constants of the equation off the plane, and become the constants of our model namely our mountain range consists of finding $2$ times this mountain ($x$), minus $4$ times the other mountain (y) plus one times the last mountain (z)\n",
    "\n",
    "![rbf_kernel_11.png](pics/rbf_kernel_11.png)\n",
    "\n",
    "And the line that separates is at height $-1$ for the constant term \n",
    "\n",
    "![rbf_kernel_12.png](pics/rbf_kernel_12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\gamma$ Parameter\n",
    "\n",
    "How do we decide if the function tails are going to be wide or thin? This is where $\\gamma$ parameter comes in \n",
    "\n",
    "![rbf_kernel_13.png](pics/rbf_kernel_13.png)\n",
    "\n",
    "In higher dimensions, this is very similar. \n",
    "\n",
    "![rbf_kernel_14.png](pics/rbf_kernel_14.png)\n",
    "\n",
    "![rbf_kernel_15.png](pics/rbf_kernel_15.png)\n",
    "\n",
    "![rbf_kernel_16.png](pics/rbf_kernel_16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines in sklearn\n",
    "In this section, you'll use support vector machines to fit a given sample dataset.\n",
    "\n",
    "Before you do that, let's go over the tools required to build this model.\n",
    "\n",
    "For your support vector machine model, you'll be using scikit-learn's SVC class. This class provides the functions to define and fit the model to your data.\n",
    "\n",
    "```\n",
    ">>> from sklearn.svm import SVC\n",
    ">>> model = SVC()\n",
    ">>> model.fit(x_values, y_values)\n",
    "```\n",
    "\n",
    "In the example above, the `model` variable is a support vector machine model that has been fitted to the data `x_values` and `y_values`. Fitting the model means finding the best boundary that fits the training data. Let's make two predictions using the model's `predict()` function.\n",
    "\n",
    "```\n",
    ">>> print(model.predict([ [0.2, 0.8], [0.5, 0.4] ]))\n",
    "[[ 0., 1.]]\n",
    "```\n",
    "\n",
    "The model returned an array of predictions, one prediction for each input array. The first input, [0.2, 0.8], got a prediction of 0.. The second input, [0.5, 0.4], got a prediction of 1..\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "When we define the model, we can specify the hyperparameters. As we've seen in this section, the most common ones are\n",
    "\n",
    "* `C`: The C parameter.\n",
    "* `kernel`: The kernel. The most common ones are 'linear', 'poly', and 'rbf'.\n",
    "* `degree`: If the kernel is polynomial, this is the maximum degree of the monomials in the kernel.\n",
    "* `gamma` : If the kernel is rbf, this is the gamma parameter.\n",
    "For example, here we define a model with a polynomial kernel of degree 4, and a C parameter of 0.1.\n",
    "\n",
    "```\n",
    ">>> model = SVC(kernel='poly', degree=4, C=0.1)\n",
    "```\n",
    "\n",
    "### Support Vector Machines Quiz\n",
    "\n",
    "In this quiz, you'll be given with the following sample dataset, and your goal is to define a model that gives 100% accuracy on it.\n",
    "\n",
    "![svm_data.png](pics/svm_data.png)\n",
    "\n",
    "The data file can be found under the \"data/data_svm.csv\" tab in the quiz below. It includes three columns, the first 2 comprising of the coordinates of the points, and the third one of the label.\n",
    "\n",
    "The data will be loaded for you, and split into features `X` and labels `y`.\n",
    "\n",
    "You'll need to complete each of the following steps:\n",
    "\n",
    "1. Build a support vector machine model\n",
    "\n",
    "Create a support vector machine classification model using scikit-learn's SVC and assign it to the `variablemodel`.\n",
    "2. Fit the model to the data\n",
    "\n",
    "If necessary, specify some of the hyperparameters. The goal is to obtain an accuracy of 100% in the dataset. Hint: Not every kernel will work well.\n",
    "3. Predict using the model\n",
    "\n",
    "Predict the labels for the training set, and assign this list to the variable `y_pred`.\n",
    "4. Calculate the accuracy of the model\n",
    "\n",
    "For this, use the function sklearn function `accuracy_score`.\n",
    "When you hit Test Run, you'll be able to see the boundary region of your model, which will help you tune the correct parameters, in case you need them.\n",
    "\n",
    "Note: This quiz requires you to find an accuracy of 100% on the training set. Of course, this screams overfitting! If you pick very large values for your parameters, you will fit the training set very well, but it may not be the best model. Try to find the smallest possible parameters that do the job, which has less chance of overfitting, although this part won't be graded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data/data_svm.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the model and assign it to the variable model.\n",
    "# Find the right parameters for this model to achieve 100% accuracy on the dataset.\n",
    "model = SVC(kernel='rbf', gamma=27)\n",
    "\n",
    "# TODO: Fit the model.\n",
    "\n",
    "model.fit(X, y)\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y, y_pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm_fit.png](pics/svm_fit.png)\n",
    "\n",
    "## Recap \n",
    "\n",
    "In this lesson, you learned about Support Vector Machines (or SVMs). SVMs are a popular algorithm used for classification problems. You saw three different ways that SVMs can be implemented:\n",
    "\n",
    "* Maximum Margin Classifier\n",
    "* Classification with Inseparable Classes\n",
    "* Kernel Methods\n",
    "\n",
    "### 1. Maximum Margin Classifier\n",
    "\n",
    "When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points (called the support vectors). For this reason, we saw that in the picture below, the boundary on the left is better than the one on the right.\n",
    "\n",
    "![lineraly_separable_svm.png](pics/lineraly_separable_svm.png)\n",
    "\n",
    "### 2. Classification with Inseparable Classes\n",
    "\n",
    "Unfortunately, data in the real world is rarely completely separable as shown in the above images. For this reason, we introduced a new hyper-parameter called **C**. The C hyper-parameter determines **how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary**. The value of C ranges between 0 and infinity. When C is **large**, you are forcing your boundary to have **fewer errors** than when it is a **small value**.\n",
    "\n",
    "**Note**: when C is too large for a particular set of data, **you might not get convergence** at all because your data cannot be separated with the small number of errors allotted with such a large value of C.\n",
    "\n",
    "![C_illu_svm.png](pics/C_illu_svm.png)\n",
    "\n",
    "### 3. Kernels\n",
    "\n",
    "Finally, we looked at what makes SVMs truly powerful, kernels. Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear. Specifically, you saw two types of kernels:\n",
    "\n",
    "* polynomial\n",
    "* rbf\n",
    "\n",
    "By far the most popular kernel is the rbf kernel (which stands for radial basis function). The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another **hyper-parameter gamma**. When **gamma** is large, the outcome is similar to having a large value of **C**, that is your algorithm will attempt to classify **every point correctly**. Alternatively, small values of gamma will try to cluster in a more general way that will make more mistakes, but may perform better when it sees new data.\n",
    "\n",
    "![rbf_illu_svm.png](pics/rbf_illu_svm.png)\n",
    "\n",
    "\n",
    "## Additional Resources \n",
    "\n",
    "[Support Vector Machines are described in Introduction to Statistical Learning starting on page 337.](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf)\n",
    "\n",
    "[The wikipedia page related to SVMs](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "[The derivation of SVMs from Stanford's CS229 notes.](http://cs229.stanford.edu/notes/cs229-notes3.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
