{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will learn how to take a bunch of models and combine them together to get a better model. \n",
    "\n",
    "![ensemble_1.png](pics/ensemble_1.png)\n",
    "\n",
    "## 1. Bagging \n",
    "\n",
    "![bagging_1.png](pics/bagging_1.png)\n",
    "\n",
    "We will get each of our friends to answer the test separately and now at the end we combine them. \n",
    "\n",
    "How do we combine them? \n",
    "\n",
    "There are many ways. For example. if the answers on the tests are values, we can **average** their values. Since they are yes-no questions we can consider **voting**. So for each question we will consider which option got the most answers by our friends. \n",
    "\n",
    "## 2. Boosting\n",
    "\n",
    "Boosting is similar but it just tries harder to exploit our friend's strengths. So let's say our first friend is a philosopher and he answers all the philosophy questions, but didn't answer the science one's very well. So we pick another friend to answer those questions. But there is a sports question and none of them knows about sports, so we have a friend who knows about sports and he answers those questions. \n",
    "So all of them together they form a \"smart friend\"!\n",
    "\n",
    "\n",
    "Some notation, all the friends are named \"weak learners\" and our \"smart friend\" is called \"strong learner\". \n",
    "\n",
    "## Ensembles\n",
    "\n",
    "This whole lesson (on ensembles) is about how we can combine (or ensemble) the models you have already seen in a way that makes the combination of these models better at predicting than the individual models.\n",
    "\n",
    "Commonly the \"weak\" learners you use are decision trees. In fact the default for most ensemble methods is a decision tree in sklearn. However, you can change this value to any of the models you have seen so far.\n",
    "\n",
    "## Why Would We Want to Ensemble Learners Together?\n",
    "\n",
    "There are two competing variables in finding a well fitting machine learning model: **Bias** and **Variance**. It is common in interviews for you to be asked about this topic and how it pertains to different modeling techniques. As a first pass, the wikipedia is quite useful. However, I will give you my perspective and examples:\n",
    "\n",
    "**Bias**: When a model has high bias, this means that means it **doesn't do a good job of bending to the data**. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad.\n",
    "\n",
    "![anscombes-quartet-3.svg](pics/anscombes-quartet-3.svg)\n",
    "\n",
    "**Variance**: When a model has high variance, this means that **it changes drastically to meet the needs of every point in our dataset**. Linear models like the one above has low variance, but high bias. An example of an algorithm that tends to have high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see.\n",
    "\n",
    "![decision-tree-sketch.png](pics/decision-tree-sketch.png)\n",
    "\n",
    "By combining algorithms, we can often build models that perform better by meeting **in the middle in terms of bias and variance**. There are some other tactics that are used to combine algorithms in ways that help them perform better as well. These ideas are based on **minimizing** bias and variance based on mathematical theories, like the central limit theorem.\n",
    "\n",
    "### Introducing Randomness Into Ensembles\n",
    "\n",
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "1. **Bootstrap the data** - that is, sampling the data with replacement and fitting your algorithm to the sampled data.\n",
    "\n",
    "2. **Subset the features** - in each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n",
    "\n",
    "In fact, these are the two random components used in the next algorithm you are going to see called **random forests**.\n",
    "\n",
    "![random_forrest_1.png](pics/random_forrest_1.png)\n",
    "\n",
    "![random_forest_2.png](pics/random_forest_2.png)\n",
    "\n",
    "Since we have two votes with \"Whatsapp\" we will go for it. \n",
    "\n",
    "## Bagging\n",
    "\n",
    "We give a subset of data to the weak learners to learn. Usually those predictions are terrible, but if we have enough data they can be preform pretty well\n",
    "\n",
    "![bagging_2.png](pics/bagging_2.png)\n",
    "\n",
    "![bagging_3.png](pics/bagging_3.png)\n",
    "\n",
    "## Adaboost \n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. First weak learner tries to minimize the error \n",
    "\n",
    "![adaboost_1](pics/adaboost_1.png)\n",
    "\n",
    "2. We take the misclassified points and we make them bigger (we increase the error), so the next weak learner will focus on those more \n",
    "\n",
    "![adaboost_2](pics/adaboost_2.png)\n",
    "\n",
    "![adaboost_3](pics/adaboost_3.png)\n",
    "\n",
    "3. Let's assume that they vote as before and we get the following \n",
    "![adaboost_4](pics/adaboost_4.png)\n",
    "\n",
    "### Weighting the data \n",
    "\n",
    "Let's assign to eachc data point an initial weight of one. Before we wanted to minimize the sum of error but now we want to minimize the weighted sum of incorrectly points (whuch for now is the same). \n",
    "\n",
    "![adaboost_5.png](pics/adaboost_5.png)\n",
    "\n",
    "Let's increase the weight of the missclassified points. (increase the weights enough to make the model a 50-50)\n",
    "\n",
    "![adaboost_6.png](pics/adaboost_6.png)\n",
    "\n",
    "Let's learn a second weak learner and increase the weights of the missclassified points again. \n",
    "\n",
    "![adaboost_7.png](pics/adaboost_7.png)\n",
    "\n",
    "Let's do it once again, we could keep that going but we will stop here. Here are the three weak learners we have created: \n",
    "\n",
    "![adaboost_8.png](pics/adaboost_8.png)\n",
    "\n",
    "### Weighting the Models\n",
    "\n",
    "In order to weight the models we need to know how well they are doing. \n",
    "\n",
    "Which is the worst model in terms of giving us information? \n",
    "\n",
    "![adaboost_9.png](pics/adaboost_9.png)\n",
    "\n",
    "**Solution** The one that tells the truth half of the time\n",
    "\n",
    "How are we going to weight those models? Here is how:\n",
    "\n",
    "![adaboost_10.png](pics/adaboost_10.png)\n",
    "\n",
    "So how would the distribution of the weights look like? \n",
    "\n",
    "![adaboost_12.png](pics/adaboost_12.png)\n",
    "\n",
    "Here is the formula for the weights:\n",
    "\n",
    "![adaboost_13.png](pics/adaboost_13.png)\n",
    "\n",
    "Can you calculate the weights of the models based on the formula above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9459101490553132\n",
      "0.0\n",
      "-1.0986122886681098\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "weight_1 = log(7/1)\n",
    "print(weight_1)\n",
    "\n",
    "weight_2 = log(4/4)\n",
    "print(weight_2)\n",
    "\n",
    "weight_3 = log(2/6)\n",
    "print(weight_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when the model is perfect?\n",
    "\n",
    "![adaboost_15](pics/adaboost_15.png)\n",
    "\n",
    "So let's see the weights of the first example \n",
    "\n",
    "![adaboost_16.png](pics/adaboost_16.png)\n",
    "\n",
    "![adaboost_17.png](pics/adaboost_17.png)\n",
    "![adaboost_18.png](pics/adaboost_18.png)\n",
    "![adaboost_19.png](pics/adaboost_19.png)\n",
    "![adaboost_20.png](pics/adaboost_20.png)\n",
    "![adaboost_21.png](pics/adaboost_21.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost in sklearn\n",
    "\n",
    "Building an AdaBoost model in sklearn is no different than building any other model. You can use scikit-learn's `AdaBoostClassifier` class. This class provides the functions to define and fit the model to your data\n",
    "\n",
    "```\n",
    ">>> from sklearn.ensemble import AdaBoostClassifier\n",
    ">>> model = AdaBoostClassifier()\n",
    ">>> model.fit(x_train, y_train)\n",
    ">>> model.predict(x_test)\n",
    "```\n",
    "\n",
    "In the example above, the `model` variable is a decision tree model that has been fitted to the data `x_train` and `y_train`. The functions `fit` and `predict` work exactly as before.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "When we define the model, we can specify the hyperparameters. In practice, the most common ones are\n",
    "\n",
    "`base_estimator`: The model utilized for the weak learners (Warning: Don't forget to import the model that you decide to use for the weak learner).\n",
    "`n_estimators`: The maximum number of weak learners used.\n",
    "For example, here we define a model which uses decision trees of max_depth 2 as the weak learners, and it allows a maximum of 4 of them.\n",
    "\n",
    "```\n",
    ">>> from sklearn.tree import DecisionTreeClassifier\n",
    ">>> model = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=2), n_estimators = 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Mission ##\n",
    "\n",
    "You recently used Naive Bayes to classify spam in this [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). In this notebook, we will expand on the previous analysis by using a few of the new techniques you've learned throughout this lesson.\n",
    "\n",
    "\n",
    "> Let's quickly re-create what we did in the previous Naive Bayes Spam Classifier notebook. We're providing the essential code from that previous workspace here, so please run this cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9885139985642498\n",
      "Precision score:  0.9720670391061452\n",
      "Recall score:  0.9405405405405406\n",
      "F1 score:  0.9560439560439562\n"
     ]
    }
   ],
   "source": [
    "# Import our libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Read in our dataset\n",
    "df = pd.read_table('data/SMSSpamCollection',\n",
    "                   sep='\\t', \n",
    "                   header=None, \n",
    "                   names=['label', 'sms_message'])\n",
    "\n",
    "# Fix our response value\n",
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "# Split our dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], \n",
    "                                                    df['label'], \n",
    "                                                    random_state=1)\n",
    "\n",
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)\n",
    "\n",
    "# Instantiate our model\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Fit our model to the training data\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = naive_bayes.predict(testing_data)\n",
    "\n",
    "# Score our model\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turns Out...\n",
    "\n",
    "We can see from the scores above that our Naive Bayes model actually does a pretty good job of classifying spam and \"ham.\"  However, let's take a look at a few additional models to see if we can't improve anyway.\n",
    "\n",
    "Specifically in this notebook, we will take a look at the following techniques:\n",
    "\n",
    "* [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "* [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "* [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "\n",
    "Another really useful guide for ensemble methods can be found [in the documentation here](http://scikit-learn.org/stable/modules/ensemble.html).\n",
    "\n",
    "These ensemble methods use a combination of techniques you have seen throughout this lesson:\n",
    "\n",
    "* **Bootstrap the data** passed through a learner (bagging).\n",
    "* **Subset the features** used for a learner (combined with bagging signifies the two random components of random forests).\n",
    "* **Ensemble learners** together in a way that allows those that perform best in certain areas to create the largest impact (boosting).\n",
    "\n",
    "\n",
    "In this notebook, let's get some practice with these methods, which will also help you get comfortable with the process used for performing supervised machine learning in Python in general.\n",
    "\n",
    "Since you cleaned and vectorized the text in the previous notebook, this notebook can be focused on the fun part - the machine learning part.\n",
    "\n",
    "### This Process Looks Familiar...\n",
    "\n",
    "In general, there is a five step process that can be used each time you want to use a supervised learning method (which you actually used above):\n",
    "\n",
    "1. **Import** the model.\n",
    "2. **Instantiate** the model with the hyperparameters of interest.\n",
    "3. **Fit** the model to the training data.\n",
    "4. **Predict** on the test data.\n",
    "5. **Score** the model by comparing the predictions to the actual values.\n",
    "\n",
    "Follow the steps through this notebook to perform these steps using each of the ensemble methods: **BaggingClassifier**, **RandomForestClassifier**, and **AdaBoostClassifier**.\n",
    "\n",
    "> **Step 1**: First use the documentation to `import` all three of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Bagging, RandomForest, and AdaBoost Classifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,  AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 2:** Now that you have imported each of the classifiers, `instantiate` each with the hyperparameters specified in each comment.  In the upcoming lessons, you will see how we can automate the process to finding the best hyperparameters.  For now, let's get comfortable with the process and our new algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BaggingClassifier with:\n",
    "# 200 weak learners (n_estimators) and everything else as default values\n",
    "bagging_classifier = BaggingClassifier(n_estimators=200)\n",
    "\n",
    "\n",
    "# Instantiate a RandomForestClassifier with:\n",
    "# 200 weak learners (n_estimators) and everything else as default values\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Instantiate an a AdaBoostClassifier with:\n",
    "# With 300 weak learners (n_estimators) and a learning_rate of 0.2\n",
    "adaboost_classifier=AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 3:** Now that you have instantiated each of your models, `fit` them using the **training_data** and **y_train**.  This may take a bit of time, you are fitting 700 weak learners after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.2,\n",
       "                   n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit your BaggingClassifier to the training data\n",
    "bagging_classifier.fit(training_data, y_train)\n",
    "\n",
    "# Fit your RandomForestClassifier to the training data\n",
    "random_forest_classifier.fit(training_data, y_train)\n",
    "\n",
    "\n",
    "# Fit your AdaBoostClassifier to the training data\n",
    "adaboost_classifier.fit(training_data, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using BaggingClassifier on the test data\n",
    "bg_y_test = bagging_classifier.predict(testing_data)\n",
    "\n",
    "# Predict using RandomForestClassifier on the test data\n",
    "rf_y_test = random_forest_classifier.predict(testing_data)\n",
    "\n",
    "\n",
    "# Predict using AdaBoostClassifier on the test data\n",
    "ad_y_test = adaboost_classifier.predict(testing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 5:** Now that you have made your predictions, compare your predictions to the actual values using the function below for each of your models - this will give you the `score` for how well each of your models is performing. It might also be useful to show the Naive Bayes model again here, so we can compare them all side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, preds, model_name=None):\n",
    "    '''\n",
    "    INPUT:\n",
    "    y_true - the y values that are actually true in the dataset (NumPy array or pandas series)\n",
    "    preds - the predictions for those values from some model (NumPy array or pandas series)\n",
    "    model_name - (str - optional) a name associated with the model if you would like to add it to the print statements \n",
    "    \n",
    "    OUTPUT:\n",
    "    None - prints the accuracy, precision, recall, and F1 score\n",
    "    '''\n",
    "    if model_name == None:\n",
    "        print('Accuracy score: ', format(accuracy_score(y_true, preds)))\n",
    "        print('Precision score: ', format(precision_score(y_true, preds)))\n",
    "        print('Recall score: ', format(recall_score(y_true, preds)))\n",
    "        print('F1 score: ', format(f1_score(y_true, preds)))\n",
    "        print('\\n\\n')\n",
    "    \n",
    "    else:\n",
    "        print('Accuracy score for ' + model_name + ' :' , format(accuracy_score(y_true, preds)))\n",
    "        print('Precision score ' + model_name + ' :', format(precision_score(y_true, preds)))\n",
    "        print('Recall score ' + model_name + ' :', format(recall_score(y_true, preds)))\n",
    "        print('F1 score ' + model_name + ' :', format(f1_score(y_true, preds)))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Bagging : 0.9755922469490309\n",
      "Precision score Bagging : 0.9217877094972067\n",
      "Recall score Bagging : 0.8918918918918919\n",
      "F1 score Bagging : 0.9065934065934066\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score for Random Forest : 0.9798994974874372\n",
      "Precision score Random Forest : 1.0\n",
      "Recall score Random Forest : 0.8486486486486486\n",
      "F1 score Random Forest : 0.9181286549707602\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score for AdaBoost : 0.9770279971284996\n",
      "Precision score AdaBoost : 0.9693251533742331\n",
      "Recall score AdaBoost : 0.8540540540540541\n",
      "F1 score AdaBoost : 0.9080459770114943\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score for Naive Bayes : 0.9885139985642498\n",
      "Precision score Naive Bayes : 0.9720670391061452\n",
      "Recall score Naive Bayes : 0.9405405405405406\n",
      "F1 score Naive Bayes : 0.9560439560439562\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Bagging scores\n",
    "print_metrics(y_test, bg_y_test, model_name=\"Bagging\")\n",
    "\n",
    "# Print Random Forest scores\n",
    "print_metrics(y_test, rf_y_test, model_name=\"Random Forest\")\n",
    "\n",
    "\n",
    "# Print AdaBoost scores\n",
    "print_metrics(y_test, ad_y_test, model_name=\"AdaBoost\")\n",
    "\n",
    "\n",
    "# Naive Bayes Classifier scores\n",
    "print_metrics(y_test, predictions, model_name=\"Naive Bayes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "Now you have seen the whole process for a few ensemble models! \n",
    "\n",
    "1. **Import** the model.\n",
    "2. **Instantiate** the model with the hyperparameters of interest.\n",
    "3. **Fit** the model to the training data.\n",
    "4. **Predict** on the test data.\n",
    "5. **Score** the model by comparing the predictions to the actual values.\n",
    "\n",
    "And that's it.  This is a very common process for performing machine learning.\n",
    "\n",
    "\n",
    "### But, Wait...\n",
    "\n",
    "You might be asking - \n",
    "\n",
    "* What do these metrics mean? \n",
    "\n",
    "* How do I optimize to get the best model?  \n",
    "\n",
    "* There are so many hyperparameters to each of these models, how do I figure out what the best values are for each?\n",
    "\n",
    "**This is exactly what the last two lessons of this course on supervised learning are all about.**\n",
    "\n",
    "\n",
    "## Recap\n",
    "\n",
    "In this lesson, you learned about a number of techniques used in ensemble methods. Before looking at the techniques, you saw that there are two variables with tradeoffs **Bias** and **Variance**.\n",
    "\n",
    "**High Bias, Low Variance** models tend to **underfit** data, as they are not flexible. **Linear models** fall into this category of models.\n",
    "\n",
    "**High Variance, Low Bias** models tend to **overfit** data, as they are **too flexible**. **Decision trees** fall into this category of models.\n",
    "\n",
    "### Ensemble Models\n",
    "In order to find a way to **optimize for both variance and bias**, we have ensemble methods. Ensemble methods have become some of the most popular methods used to compete in competitions on Kaggle and used in industry across applications.\n",
    "\n",
    "There were two **randomization techniques** you saw to **combat overfitting**:\n",
    "\n",
    "* **Bootstrap the data** - that is, sampling the data with replacement and fitting your algorithm and fitting your algorithm to the sampled data.\n",
    "\n",
    "* **Subset the features** - in each split of a decision tree or with each algorithm used an ensemble only a subset of the total possible features are used.\n",
    "\n",
    "### Techniques\n",
    "You saw a number of ensemble methods in this lesson including:\n",
    "\n",
    "* [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "* {RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "* [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "\n",
    "Another really useful guide for ensemble methods can be [found in the documentation here](http://scikit-learn.org/stable/modules/ensemble.html). These methods can also all be extended to regression problems, not just classification.\n",
    "\n",
    "Additional Resources\n",
    "Additionally, here are some great resources on AdaBoost if you'd like to learn some more!\n",
    "\n",
    "Here is the original paper from Freund and Schapire.\n",
    "A follow-up paper from the same authors regarding several experiments with Adaboost.\n",
    "A great tutorial by Schapire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
