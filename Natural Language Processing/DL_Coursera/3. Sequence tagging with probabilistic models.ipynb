{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hidden Markov Models\n",
    "\n",
    "![hmm_1.png](pics/hmm_1.png)\n",
    "![hmm_2.png](pics/hmm_2.png)\n",
    "![hmm_2.png](pics/hmm_3.png)\n",
    "![hmm_2.png](pics/hmm_4.png)\n",
    "![hmm_2.png](pics/hmm_5.png)\n",
    "![hmm_2.png](pics/hmm_6.png)\n",
    "![hmm_2.png](pics/hmm_7.png)\n",
    "![hmm_2.png](pics/hmm_8.png)\n",
    "![hmm_2.png](pics/hmm_9.png)\n",
    "![hmm_2.png](pics/hmm_10.png)\n",
    "![hmm_2.png](pics/hmm_11.png)\n",
    "![hmm_2.png](pics/hmm_12.png)\n",
    "![hmm_2.png](pics/hmm_13.png)\n",
    "![hmm_2.png](pics/hmm_14.png)\n",
    "\n",
    "But the thing is that in real life, usually, you do not have tags in your training data. So the only thing that you'll see is plain text, and you still need to train hidden Markov model somehow. How can you do it? Well, obviously, you cannot estimate those indicator functions because you don't see the tags of the positions, but you could try to approximate those indicators by some probabilities. So compare the formula on the slide above with the formula in the bottom of the slide below. The only thing that has changed is that instead of indicators, I have probabilities now. So something in between zero and one, but how can we get these probabilities? Well, if they have some trained hidden Markov model, we could try to apply it to our texts to produce those probabilities.\n",
    "\n",
    "So the **E-step** in the top of this slide says that given some trained hidden Markov model, we can produce the probability to see tags $S_i$ and $S_j$ in the position $t$. So this is something like three-dimensional array. $t$, $i$, and $j$ would be the indices, and it can be actually done effectively with dynamic programming. The only thing is that we need to have trained model there. So the clever idea is to alternate two steps. The E-step says that let us fix some current parameters of the Model $A$ and $B$ matrices and use them to generate those estimates for probabilities of tags for every certain position. And **M-step** says let us fix those estimates and use them to update our parameters, and those parameters updates are actually still maximum likelihood estimates in this case. \n",
    " \n",
    "![hmm_2.png](pics/hmm_15.png)\n",
    "\n",
    "In this slide it is shown only how to update A matrix, but you can do something similar for B matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Viterbi algorithm: what are the most probable tags?\n",
    "\n",
    "![hmm_2.png](pics/hmm_16.png)\n",
    "So the sequence of text in the top and in the bottom of the slides are both valid. So both sequences could generate this piece of text, right? This is because we have very ambiguous language. For example, likes can be noun, or verb, or something else maybe, if we think about it.\n",
    " \n",
    "Okay, then how can we generate the most probable sequence of text given a piece of text?\n",
    "\n",
    "This is called decoding problem, and this is formalized in the bottom of the slide. So we need to find y which maximizes the probability of y given x.\n",
    "\n",
    "![hmm_2.png](pics/hmm_17.png)\n",
    "\n",
    "Now could we probably just compute the probabilities of all ys, and then choose the best one?\n",
    "\n",
    "Well not actually, because this brute force approach would be really slow, right?  Fortunately there is an algorithm based on dynamic programming that can help us to do this effectively.\n",
    "\n",
    "![hmm_2.png](pics/hmm_18.png)\n",
    "![hmm_2.png](pics/hmm_19.png)\n",
    "![hmm_2.png](pics/hmm_20.png)\n",
    "A Matrix let's say that this row is just filled with equal probabilities:\n",
    "![hmm_2.png](pics/hmm_21.png)\n",
    "\n",
    "B Matrix which tells us the probabilities of the output tokens given some states. :\n",
    "![hmm_2.png](pics/hmm_22.png)\n",
    "![hmm_2.png](pics/hmm_23.png)\n",
    "![hmm_2.png](pics/hmm_24.png)\n",
    "![hmm_2.png](pics/hmm_25.png)\n",
    "![hmm_2.png](pics/hmm_26.png)\n",
    "![hmm_2.png](pics/hmm_27.png)\n",
    "![hmm_2.png](pics/hmm_28.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MEMMs, CRFs and other sequential models for Named Entity Recognition\n",
    "\n",
    "![hmm_2.png](pics/hmm_29.png)\n",
    "\n",
    "You maybe remember the formula, and one important thing to tell you is that it is generative model, which means that it models the joint probabilities of x and y. So you see that every arrow is about some particular probability in the formula. So we have transition probabilities going from one y to the next one, and we have output probabilities that go to x variables. \n",
    "\n",
    "![hmm_2.png](pics/hmm_30.png)\n",
    "\n",
    "Now, another one would be Maximum Entropy Markov model. Is a super similar. So, do you see what is now different in the picture? Only one thing has changed. You have now the arrows going from x to ys. \n",
    "\n",
    "What is also important is that this model is discriminative, which means that it models the conditional probability of y given x. So, it doesn't care to describe how the text can be generated. It says that the text is observable and we just need to produce the probabilities for our hidden variables, y.\n",
    "\n",
    "Now, another important thing to mention is that you see that it is factorized nicely still. Right? So you have some separate probabilities, and you have a product of this probabilities. Let us look into more details how every probability can be written down.\n",
    "\n",
    "![hmm_2.png](pics/hmm_31.png)\n",
    "\n",
    "So, we have some exponents and we have some normalization constants. So, this is actually just soft marks. This is just soft marks applied to some brackets. What do we see in the brackets? We have there something linear. We have weights multiplied by features. So you can think about a vector of weights and the vector of features, and you have a dot product. \n",
    "\n",
    "Do you remember a similar model? Well, actually logistic regression is super similar to maximum entropy Markov model. There, you also have a soft max applied to dot product of features and weights. The only difference is that here, you have rather complicated features that can depend on the next and the previous states. So, the model knows about the sequence, and it knows that the tags are not just separate.\n",
    "\n",
    "Let us write down one more probabilistic graphical model that can be even more powerful than that one. \n",
    "\n",
    "![hmm_2.png](pics/hmm_32.png)\n",
    "\n",
    "![hmm_2.png](pics/hmm_33.png)\n",
    "\n",
    "Do you see an important difference between CRF and maximum entropy Markov model? The thing is that now you have only one normalization constrain that goes outside of the product. So you don't have any probabilities inside at all. So, the model is not factorized into probabilities. Instead, we have some product of some energy functions, and then we normalize all of them to get the final probability.\n",
    "\n",
    "And this normalization is actually complicated because, well, we have many different sequences, and we have to normalize in such a way that these probability sums to one over all possible sequences of tags. Now, when we depict this model with the graph, it would be undirected graph. So, I don't have any arrows at all. I have just some links between the nodes. \n",
    "\n",
    "n the top of the slide. So here, in the top of the slide, you can see that your features can depend on three things. And here I kept them only for two things. So I have one type of features about transitions and another type of features about outputs. \n",
    "\n",
    "\n",
    "Obviously, I could have something more complicated. \n",
    "\n",
    "![hmm_2.png](pics/hmm_34.png)\n",
    "\n",
    "So, a general form of conditional random field would look like that. So you have some arbitrary factors that depend on some groups of y variables and x variables. \n",
    "\n",
    "But good thing is that we don't need to know the specidics because there are many black box tools to help us out : \n",
    "\n",
    "![hmm_2.png](pics/hmm_35.png)\n",
    "\n",
    "The important thing is how to generate features to feed them into those models. \n",
    "\n",
    "![hmm_2.png](pics/hmm_36.png)\n",
    "\n",
    "From the formulas, you might remember that those \"f\" features can depend on three things; \n",
    "1. the current tag, \n",
    "2. the previous tag, \n",
    "3. and the current output.\n",
    "\n",
    "Now, not to be overwhelmed with the variety of your features, there is a very nice common technique which is called label observation features. So, it says that you are only allowed to have these kind of features. \n",
    "\n",
    "The observation part is about something that depends on the output.\n",
    "\n",
    "So, we will go to this part, and the green part, the labeled part, is about indicators. So you just check whether you have the current label equal to y, and you check it for all possible labels. So, it means that you have as many features as many labels you have multiplied by the number of different observation functions that you invent.\n",
    "\n",
    "And in the case of the second and the third line, you will have even more features because there, you check these indicators for the current and for the previous tags. So, we are going to have lots of them. Now, how those observation parts will look like.\n",
    "\n",
    "![hmm_2.png](pics/hmm_37.png)\n",
    "\n",
    "For example, you can check whether your word is capitalized, or whether it has a dash in the middle, or whether it consists only from capital letters, or whether it appears in some predefined list of stop words or predefined list of some names. \n",
    "\n",
    "So, actually this is how your work would look like if you decided to use CRF for some sequence tagging task. You would take some implementation and you would generate as many useful features as you can think about. \n",
    "\n",
    "![hmm_2.png](pics/hmm_38.png)\n",
    "\n",
    "Now, one trick that I want to mention here is that even though we say that the feature can depend only on the current output, $x_t$, well, it is not like that. So, honestly, we can put into this $x_t$ everything that we want.\n",
    "\n",
    "For example, we can say that our current $x_t$ consists of the **current word, the previous word and the next word**, and with have features for all of them. So, you should multiply those tremendous number of features by three right now. And it is okay. So, the model will not break down just because it is discriminative model. So, we do not care about modeling the sequence of x which means that we can do whatever we want basically.\n",
    "\n",
    "![hmm_2.png](pics/hmm_39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilities of tag sequences in HMMs\n",
    "\n",
    "In this reading we recap hidden Markov models. We demonstrate how to compute probabilities of tag sequences and re-estimate parameters with Baum-Welch algorithm. \n",
    "\n",
    "## Hidden Markov Model\n",
    "\n",
    "Consider the following rhyme from \"Jabberwocky\" by Lewis Caroll:\n",
    "\n",
    "’Twas brillig, and the slithy toves\n",
    "\n",
    "Did gyre and gimble in the wabe;\n",
    "\n",
    "All mimsy were the borogoves,\n",
    "\n",
    "And the mome raths outgrabe.\n",
    "\n",
    "Let's take the last phrase, \"the mome raths outgrabe\", as an example. Let's build a hidden Markov model for predicting part of speech tags in this sentence. For simplicity, suppose that we have just three tags: N (noun), V (verb) and D (determiner). We need to specify initial probabilities of these tags and transition probabilities from one tag to another. Surely, these probabilities may be estimated using some annotated corpus. But let's suppose for now that all tags and all transitions are equiprobable:\n",
    "\n",
    "\n",
    "\n",
    "$$p(N | start) = p(O | start) = p(D | start) = 1/3$$\n",
    "\n",
    "$$p(N| N) = p(V | N) = p(D | N) = 1/3$$\n",
    "\n",
    "and so on.\n",
    "\n",
    "Dealing with HMMs, we also need to specify output probabilities of words given the tag. For simplicity, we consider the following outcomes:\n",
    "\n",
    "N: mome | raths | outgrabe\n",
    "\n",
    "V: raths\n",
    "\n",
    "D: the | a\n",
    "\n",
    "Let all these outcomes be also equiprobable, i. e,\n",
    "\n",
    "$$p(mome | N) = p(raths | N) = p(outgrabe | N) = 1/3$$\n",
    "\n",
    "$$p(raths | V) = 1$$\n",
    "\n",
    "$$p(the | D) = p (a | D) = 1/2$$\n",
    "\n",
    "Note that some words like \"raths\" may be generated from different tags (otherwise, the tagging is trivial). Note also, that our test phrase does not have to contain all of these words. For example, \"a\" is absent in the test sentence.\n",
    "\n",
    "## Probabilities of tag sequences\n",
    "\n",
    "Given this toy model, let's find the probabilities of possible tag sequences for the phrase \"the mome raths outgrabe\". In other words, these are the conditional probabilities: $p(XXXX | phrase)$, where each tag $X$ is either $N$, or $V$, or $D$.\n",
    "\n",
    "**First question for you: how many different tag sequences exist? Second question: which of them could happen in our case with the transition and output probabilities defined above?**\n",
    "\n",
    "Answers: there are $3^4 = 81$ sequences, but only two of them are possible in our case. \"the\" can be generated only from $D$, \"mome\" and \"outgrabe\" can be generated only from $N$, and \"raths\" can be generated wither from $N$ or $V$. So we can have either $DNNN$ or $DNVN$.\n",
    "\n",
    "**So we have just seen, that probabilities of 79 tag sequences are equal to 0**, and we need to compute these two: $p(DNNN | phrase)$ and $p(DNVN | phrase)$. According to the HMM model, the joint probabilities are:\n",
    "\n",
    "$$p(DNVN, phrase) = p(D | start)\\, p(the | D)\\, p(N | D)\\, p(mome | N)\\, p(V | N)\\, p(raths |V)\\, p(N |V) \\, p(outgrabe | N)$$\n",
    "\n",
    "$$p(DNNN, phrase) = p(D | start)\\, p(the | D)\\, p(N | D)\\, p(mome | N) \\, \\boldsymbol{p(N | N)\\, p(raths |N) \\, p(N |N)} \\, p(outgrabe | N)$$\n",
    "\n",
    "At this point, you could just use the given values above to compute these expressions. After that, you would need to normalize them in such a way that they sum into 1, since it should be a distribution, and all other 79 values are known to be 0.\n",
    "\n",
    "But let's reduce our calculations and look closer into the two formulas above. The only term that differs there is this one: $p(raths |N) = 1/3$ while $p(raths|V) = 1$. So, for some multiplier x,\n",
    "\n",
    "$p(DNNN | phrase) = x \\cdot 1/3$\n",
    "\n",
    "$p(DNVN | phrase) = x \\cdot 1$\n",
    "\n",
    "Since these probabilities must sum into one, we find that they are equal to $1/4$ and $3/4$, and we are done.\n",
    "\n",
    "## Baum-Welch probability re-estimation\n",
    "\n",
    "Now let's see how to re-estimate transition or output probabilities in our model, given the same sentence \"the mome raths outgrabe\". It means performing one iteration of Baum-Welch algorithm (= EM-algorithm). Actually, we have just done **E-step** by computing probabilities of tag sequences. Now let's see how the **M-step** works. For example, let's re-estimate the transition probability $p(V|N)$, that used to be one-third.\n",
    "\n",
    "We need to find the **expectation of the number of transitions from N to V** and divide it to **the expectation of the number of transitions from N to any tag**. The expectation is taken with respect to the probabilities of tag sequences (computed above).\n",
    "\n",
    "Remember, there are only two possible sequences. We have 0 transitions from N to V in the DNNN sequence and exactly one such transition in the DNVN sequence. So the expectation for (N -> V) transitions is $1/4 \\cdot 0 + 3/4 \\cdot 1 = 3/4$.\n",
    "\n",
    "Similarly, we have two transitions from N to some tag in DNNN and 1 such transition in DNVN. The expectation for (N -> ?) transitions is $1/4 \\cdot 2 + 3/4 \\cdot 1 = 5/4$.\n",
    "\n",
    "Thus, the new estimation for the transition probability $p(V | N)$ is $3/5$. This is exactly the probability, that would be assigned to the corresponding HMM parameter if we were training it with Baum-Welch on this one phrase.\n",
    "\n",
    "In real examples though, the flow of computations in Baum-Welch is a bit different. First, usually you have too many tag sequences. So it's impossible to compute there probabilities and take the expectation with respect to them. E.g. imagine you have 20 possible tags and a sequence of length 10. Then you would have $20^{10}$ tag sequences! So instead of the probabilities of the whole sequences, you would **compute the probabilities of two sequential tags** (see the slides). These probabilities would be enough to perform M-step. Second, it is **hard to compute even these tag pair probabilities**, so here a so called Forward-Backward algorithm is used. It's a dynamic programming algorithm that allows efficient computations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Sequence tagging with probabilistic models \n",
    "\n",
    "![hmm_quiz_1.png](pics/hmm_quiz_1.png)\n",
    "![hmm_quiz_1.png](pics/hmm_quiz_2.png)\n",
    "![hmm_quiz_1.png](pics/hmm_quiz_3.png)\n",
    "![hmm_quiz_1.png](pics/hmm_quiz_4.png)\n",
    "![hmm_quiz_1.png](pics/hmm_quiz_5.png)\n",
    "\n",
    "Because:\n",
    "$$p(ONVON | phase) = p(ONVOV | phase) = 3/10$$\n",
    "$$p(OOVON | phase) = p(OOVOV | phase) = 2/10$$\n",
    "\n",
    "If you did it along the lines of our reading, this would come from the equation: $x \\cdot \\dfrac{1}{2} + x \\cdot \\dfrac{1}{2} + x \\cdot \\dfrac{1}{3} + x \\cdot \\dfrac{1}{3} = 1$, since the only term that differs for the sequences is $p(mimsy|N)$ versus $p(mimsy|O)$.\n",
    "\n",
    "Once you have the tag probabilities, all you need is to compute two math expectations for the transitions (O->N) and (O->?). For each of them, go through all 4 possible tag sequences, count how many transitions you have in each case, and weight those counts with the probabilities above. \n",
    "\n",
    "$$E(O->N) = 0 \\cdot \\dfrac{3}{10} + 1 \\cdot \\dfrac{3}{10} + 1 \\cdot \\dfrac{2}{10} + 2 \\cdot \\dfrac{2}{10} = \\dfrac{9}{10}$$\n",
    "\n",
    "$$E(O->?) = 2 \\cdot \\dfrac{3}{10} + 2 \\cdot \\dfrac{3}{10} + 3 \\cdot \\dfrac{2}{10} + 3 \\cdot \\dfrac{2}{10} = \\dfrac{24}{10}$$\n",
    " \n",
    "$$\\dfrac{E(O->N)}{E(O->?)} =  \\dfrac{9}{24} = 0.375$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
