{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Projection \n",
    "\n",
    "![r_p_1.png](pics/r_p_1.png)\n",
    "\n",
    "![r_p_1.png](pics/r_p_2.png)\n",
    "\n",
    "![r_p_1.png](pics/r_p_3.png)\n",
    "\n",
    "![r_p_1.png](pics/r_p_4.png)\n",
    "\n",
    "Paper: [Random projection in dimensionality reduction: Applications to image and text data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8124&rep=rep1&type=pdf)\n",
    "\n",
    "This paper examines using Random Projection to reduce the dimensionality of image and text data. It shows how Random Projection proves to be a computationally simple method of dimensionality reduction, while still preserving the similarities of data vectors to a high degree. The paper shows this on real-world datasets including noisy and noiseless images of natural scenes, and text documents from a newsgroup corpus.\n",
    "\n",
    "Paper: [Random Projections for k-means Clustering](https://papers.nips.cc/paper/3901-random-projections-for-k-means-clustering.pdf)\n",
    "\n",
    "This paper uses Random Projection as an efficient dimensionality reduction step before conducting k-means clustering on a dataset of 400 face images of dimensions 64 × 64.\n",
    "\n",
    "![r_p_1.png](pics/r_p_5.png)\n",
    "\n",
    "![r_p_1.png](pics/r_p_6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis\n",
    "\n",
    "A method similar to PCA is ICA, it takes a set of features and produces a different set that is useful in some way. But it's different in that PCA works to maximize variance, ICA assumes that the features are mixtures of independent sources and it tries to isolate these independent sources that are mixed in this dataset. \n",
    "\n",
    "Let's look at the example of cocktail party: \n",
    "\n",
    "![i_c_a_1.png](pics/i_c_a_1.png)\n",
    "![i_c_a_1.png](pics/i_c_a_2.png)\n",
    "![i_c_a_1.png](pics/i_c_a_3.png)\n",
    "![i_c_a_1.png](pics/i_c_a_4.png)\n",
    "\n",
    "This is a type of problem called **Blind source separation**\n",
    "\n",
    "## ICA Algorithm\n",
    "\n",
    "![ica_alg_1.png](pics/ica_alg_1.png)\n",
    "\n",
    "![ica_alg_1.png](pics/ica_alg_2.png)\n",
    "The goal is to find the best W. \n",
    "\n",
    "Paper: \"[Independent component analysis: algorithms and applications](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.322.679&rep=rep1&type=pdf)\" (pdf)\n",
    "\n",
    "![ica_alg_1.png](pics/ica_alg_3.png)\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "1. The components are statistically independent\n",
    "2. Components have non-Gaussian distributions (very important here)\n",
    "\n",
    "Non-Gaussian distribution of components is the key to estimating ICA and without it, we'll not be able to calculate, we'll not be able to store the original signals if they were Gaussian. \n",
    "\n",
    "So building from here, \n",
    "1. the Central Limit Theory tells us that the distribution of a sum of independent variables tends towards a Gaussian distribution. \n",
    "2. So, knowing that, we take W to be a matrix that maximizes the non-Gaussianity of W transpose X. In this case we need to calculate non-Gaussianity because that is the term the whole algorithm tries to maximize. So, what is one way to calculate non-Gaussianity?\n",
    "3. The term $w^{+} = E\\{xg(w^{T}x)\\} - E\\{g´(w^{T}x)\\}w$ is an approximation of something called **negentropy** (it comes from Information Theory), you don't need to know all these details, as long as you know the assumptions of non-Gaussianity. \n",
    "\n",
    "![ica_quiz_1.png](pics/ica_quiz_1.png)\n",
    "![ica_quiz_1.png](pics/ica_quiz_2.png)\n",
    "\n",
    "## Applications\n",
    "\n",
    "Paper: [Independent Component Analysis of Electroencephalographic Data](http://papers.nips.cc/paper/1091-independent-component-analysis-of-electroencephalographic-data.pdf)\n",
    "\n",
    "This paper is an example of how ICA is used to transform EEG scan data to do blind source separation. For example, on the left are the readings of 14 channels from an EEG scan that lasted 4.5 seconds. On the right are the independent components extracted from that dataset:\n",
    "\n",
    "![eeg-ica.png](pics/eeg-ica.png)\n",
    "\n",
    "Paper: [Applying Independent Component Analysis to Factor Model in Finance](https://pdfs.semanticscholar.org/a34b/e08a20eba7523600203a32abb026a8dd85a3.pdf) [PDF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
