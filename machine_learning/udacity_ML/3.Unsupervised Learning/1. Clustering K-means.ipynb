{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Outline\n",
    "\n",
    "Unsupervised learning is all about understanding how to group our data when we either\n",
    "\n",
    "1. Do not have a label to predict. An example of this is using an algorithm to look at brain scans to find areas that may raise concern. You don't have labels on the images to understand what areas might raise reason for concern, but you can understand which areas are most similar or different from one another.\n",
    "\n",
    "2. Are not trying to predict a label, but rather group our data together for some other reason! One example of this is when you have tons of data, and you would like to condense it down to a fewer number of features to be used.\n",
    "\n",
    "# Types of Unsupervised Learning\n",
    "There are two popular methods for unsupervised machine learning.\n",
    "\n",
    "* **Clustering** - which groups data together based on similarities\n",
    "\n",
    "* **Dimensionality Reduction** - which condenses a large number of features into a (usually much) smaller set of features.\n",
    "\n",
    "# K-Means\n",
    "The K-Means algorithm is used to cluster all sorts of data.\n",
    "\n",
    "It can group together\n",
    "\n",
    "* Books of similar genres or written by the same authors.\n",
    "* Similar movies.\n",
    "* Similar music.\n",
    "* Similar groups of customers.\n",
    "\n",
    "This clustering can lead to product, movie, music and other types of recommendations.\n",
    "\n",
    "In the K-means algorithm 'k' represents the number of clusters you have in your dataset. In this video, you saw that a k value of two makes a lot of sense. There is one cluster of points with shorter distances for when I travel to work. A second cluster is created when I travel to my parents' house.\n",
    "\n",
    "![km](pics/k_m_1.png)\n",
    "\n",
    "Visually inspecting your data easily shows these two clusters. On the next page, you will have an opportunity to make sure you have this technique for finding clusters mastered.\n",
    "\n",
    "## Choosing K\n",
    "\n",
    "So far you have identified k when you can visually inspect your data to identify the number of clusters. However, in practice, you often have tons of data with many features. This can make visualizing your clusters impossible.\n",
    "\n",
    "In these cases, choosing k is often an art and a science. Often researchers have an idea of what k should be ahead of time. In other cases, no one has any idea what k should be! How do we choose k in these cases? Don't worry there is a general method used for these cases. \n",
    "\n",
    "## Elbow Method\n",
    "\n",
    "When you have no idea how many clusters exist in your dataset, a common strategy for determining k is the elbow method. In the elbow method, you create a plot of the number of clusters (on the x-axis) vs. the average distance of the center of the cluster to each point (on the y-axis). This plot is called a **scree plot**.\n",
    "\n",
    "The average distance will always decrease with each additional cluster center. However, with fewer clusters, those decreases will be more substantial. At some point, adding new clusters will no longer create a substantial decrease in the average distance. **This point is known as the elbow.**\n",
    "\n",
    "## How Does K-Means Work?\n",
    "\n",
    "You choose k as the number of clusters you believe to be in your dataset or...\n",
    "You use the elbow method to determine k for your data.\n",
    "Then this number of clusters is created within your dataset, where each point is assigned to each group.\n",
    "\n",
    "However, to understand what edge cases might occur when grouping points together, it is necessary to understand exactly what the k-means algorithm is doing. Here is one method for computing k-means:\n",
    "\n",
    "1. Randomly place k centroids amongst your data.\n",
    "\n",
    "Then within a loop until convergence perform the following two steps:\n",
    "\n",
    "2. Assign each point to the closest centroid.\n",
    "\n",
    "3. Move the centroid to the center of the points assigned to it.\n",
    "\n",
    "At the end of this process, you should have k-clusters of points.\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "For any machine learning algorithm that uses distances as a part of its optimization, it is important to scale your features.\n",
    "\n",
    "You saw this earlier in regularized forms of regression like Ridge and Lasso, but it is also true for k-means. In future sections on PCA and ICA, feature scaling will again be important for the successful optimization of your machine learning algorithms.\n",
    "\n",
    "Though there are a large number of ways that you can go about scaling your features, there are two ways that are most common:\n",
    "\n",
    "1. Normalizing or Max-Min Scaling - this type of scaling transforms variable values to between 0 and 1.\n",
    "2. Standardizing or Z-Score Scaling - this type of scaling transforms variable values so they have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "# Clustering Recap\n",
    "We just covered a bunch of information! Here is a quick recap!\n",
    "\n",
    "## I. Clustering\n",
    "You learned about clustering, a popular method for unsupervised machine learning. We looked at three ways to identify clusters in your dataset.\n",
    "\n",
    "1. **Visual Inspection** of your data.\n",
    "2. **Pre-conceived** ideas of the number of clusters.\n",
    "3. **The elbow method**, which compares the average distance of each point to the cluster center for different numbers of centers.\n",
    "\n",
    "## II. K-Means\n",
    "\n",
    "You saw the k-means algorithm for clustering data, which has 3 steps:\n",
    "\n",
    "1. Randomly place k-centroids amongst your data.\n",
    "\n",
    "Then repeat the following two steps until convergence (the centroids don't change):\n",
    "\n",
    "2. Look at the distance from each centroid to each point. Assign each point to the closest centroid.\n",
    "\n",
    "3. Move the centroid to the center of the points assigned to it.\n",
    "\n",
    "## III. Concerns with K-Means\n",
    "Finally, we discussed some concerns with the k-means algorithm. These concerns included:\n",
    "\n",
    "1. Concern: The random placement of the centroids may lead to non-optimal solutions.\n",
    "\n",
    "Solution: Run the algorithm multiple times and choose the centroids that create the smallest average distance of the points to the centroids.\n",
    "\n",
    "2. Concern: Depending on the scale of the features, you may end up with different groupings of your points.\n",
    "\n",
    "Solution: Scale the features using Standardizing, which will create features with mean 0 and standard deviation 1 before running the k-means algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
