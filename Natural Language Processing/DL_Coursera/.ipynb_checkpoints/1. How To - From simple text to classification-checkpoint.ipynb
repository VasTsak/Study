{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Pre-processing\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The process that splits an input sequence into so-called tokens. \n",
    "\n",
    "![white_token.png](pics/white_token.png)\n",
    "![white_token.png](pics/more_tokens.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is Andrew's text, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Normalization\n",
    "\n",
    "We may want the same token for different forms of the world \n",
    "* wolf, wolves -> wolf\n",
    "* talk, talks -> talk\n",
    "\n",
    "### Stemming \n",
    "* A process of removing and replacing suffixes to get to the root form of the world which is called **stem**.\n",
    "* Usually refers to hezristics that chop off suffixes\n",
    "![lemma_1.png](pics/lemma_1.png)\n",
    "\n",
    "### Lemmatization\n",
    "* Usually refers to doing things properly with the use of a vocabulary and morphological analysis\n",
    "* Returns the base or dictionary form of a word, which is known as the **lemma**\n",
    "\n",
    "\n",
    "![lemma_1.png](pics/lemma_2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"feet cats wolves talked\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet cat wolv talk'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot cat wolf talked'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![norm_problems.png](pics/norm_problems.png)\n",
    "\n",
    "## Summary\n",
    "* We can think of text as a sequence of tokens\n",
    "* Tokenization is a process of extracting those tokens\n",
    "* We can normalize tokens using stemming of lemmatization\n",
    "* We can also normalize casing and acronyms \n",
    "\n",
    "# 2. Feature Extraction from text\n",
    "\n",
    "## Bag of Words - BOW\n",
    "![bow](pics/bow.png)\n",
    "\n",
    "## N-grams\n",
    "![bow](pics/bow_1.png)\n",
    "\n",
    "![bow](pics/remove_ngrams.png)\n",
    "\n",
    "![bow](pics/remove_ngrams_1.png)\n",
    "\n",
    "There're a lot of medium frequency n-grams\n",
    "* It proved to be useful to look at n-gram frequency in our corpus for filtering out bad n-grams\n",
    "* What if we use it for ranking of medium frequency n-grams?\n",
    "* **Idea**: the n-gram with smaller frequency can be more discriminating becase it can capture a specific issue in the review.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "![tf_idf_1.png](pics/tf_idf_1.png)\n",
    "![inv_tf.png](pics/inv_tf.png)\n",
    "![tf_idf_pros.png](pics/tf_idf_pros.png)\n",
    "![tidf_code.png](pics/tidf_code.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd \n",
    "texts = [\n",
    "    \"good movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good one\"\n",
    "]\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"good movie\",\n",
    "         \"not a good movie\",\n",
    "         \"did not like\",\n",
    "         \"i like it\",\n",
    "         \"good one\"\n",
    "        ]\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* We've made simple counter features in bag of words manner\n",
    "* You can add n-grams \n",
    "* You can replace counters with TF-IDF values\n",
    "\n",
    "# 3. Linear models for Sentiment analysis\n",
    "\n",
    "We will use the Imdb data base which has 50k records with 25k positive and 25k negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sparse_data.png](pics/sparse_data.png)\n",
    "![sparse_models.png](pics/sparse_models.png)\n",
    "![lr_1.png](pics/lr_1.png)\n",
    "![lr_acc.png](pics/lr_acc.png)\n",
    "![lr_acc_1.png](pics/lr_acc_1.png)\n",
    "![lr_acc_1.png](pics/lr_acc_2.png)\n",
    "![lr_acc_1.png](pics/lr_acc_3.png)\n",
    "\n",
    "## Summary \n",
    "\n",
    "* Bag of words and simple linear models actually work for texts\n",
    "* The accuracy gain from deep learning models is not mind blowing for sentiment classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hashing trick in spam filtering \n",
    "\n",
    "![m_n_g.png](pics/m_n_g.png)\n",
    "![m_n_g_1.png](pics/m_n_g_1.png)\n",
    "![m_n_g_1.png](pics/m_n_g_2.png)\n",
    "![m_n_g_1.png](pics/m_n_g_3.png)\n",
    "![m_n_g_1.png](pics/m_n_g_4.png)\n",
    "![m_n_g_1.png](pics/m_n_g_5.png)\n",
    "![m_n_g_1.png](pics/m_n_g_6.png)\n",
    "![m_n_g_1.png](pics/m_n_g_7.png)\n",
    "![m_n_g_1.png](pics/m_n_g_8.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "* We've taken a look on applications of feature hashing \n",
    "* Personalized features is a nice trick\n",
    "* Linear models over bag of words scale well for production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Quiz - Classical text mining\n",
    "\n",
    "![quiz_1_1.png](pics/quiz_1_1.png)\n",
    "\n",
    "![quiz_1_2.png](pics/quiz_1_2.png)\n",
    "\n",
    "![quiz_1_2.png](pics/quiz_1_3.png)\n",
    "![quiz_1_2.png](pics/quiz_1_3_a.png)\n",
    "\n",
    "![quiz_1_2.png](pics/quiz_1_4.png)\n",
    "![quiz_1_2.png](pics/quiz_1_5.png)\n",
    "![quiz_1_2.png](pics/quiz_1_6.png)\n",
    "\n",
    "![fet_1.png](pics/fet_1.png)\n",
    "![fet_1.png](pics/fet_2.png)\n",
    "![fet_1.png](pics/fet_3.png)\n",
    "![fet_1.png](pics/fet_4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "text = \"gagae{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gagae  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE.sub(r' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Server - any equivalent of Excel's CHOOSE function?\n",
      "SQL Server - any equivalent of Excel's CHOOSE function?\n",
      "Wrong answer for the case: 'SQL Server - any equivalent of Excel's CHOOSE function?'\n",
      "How to free c++ memory vector<int> * arr?\n",
      "How to free c++ memory vector<int> * arr?\n",
      "Wrong answer for the case: 'How to free c++ memory vector<int> * arr?'\n"
     ]
    }
   ],
   "source": [
    "examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "answers = [\"sql server equivalent excels choose function\", \n",
    "           \"free c++ memory vectorint arr\"]\n",
    "for text, ans in zip(examples, answers):\n",
    "    print(text)\n",
    "    print(REPLACE_BY_SPACE_RE.sub(r' ',text))\n",
    "    if REPLACE_BY_SPACE_RE.sub(r' ',text) != ans:\n",
    "        print(\"Wrong answer for the case: '%s'\" % text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Neural Networks for words\n",
    "\n",
    "![bow_repr_1.png](pics/bow_repr_1.png)\n",
    "\n",
    "In neural networks we prefer a dense representation.\n",
    "\n",
    "![nn_rep_1.png](pics/nn_rep_1.png)\n",
    "\n",
    "How do you represent a sentece? A sum is an approach that works well in practice\n",
    "![nn_rep_2.png](pics/nn_rep_2.png)\n",
    "![nn_rep_3.png](pics/nn_rep_3.png)\n",
    "![nn_rep_4.png](pics/nn_rep_4.png)\n",
    "![nn_reo_4.png](pics/nn_reo_4.png)\n",
    "![nn_rep_4.png](pics/nn_rep_5.png)\n",
    "![nn_rep_4.png](pics/nn_rep_6.png)\n",
    "![nn_rep_4.png](pics/nn_rep_7.png)\n",
    "\n",
    "![m_p_ot_1.png](pics/m_p_ot_1.png)\n",
    "\n",
    "![m_p_ot_2.png](pics/m_p_ot_2.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "* You can just average pre-trained word2vec vectors for your text\n",
    "* You can do better with 1D convolutions that learn more complex features\n",
    "\n",
    "# 7. Neural Networks for words\n",
    "\n",
    "![nn_w.png](pics/nn_w.png)\n",
    "![nn_w_1.png](pics/nn_w_1.png)\n",
    "![nn_w_1.png](pics/nn_w_3.png)\n",
    "![nn_w_1.png](pics/nn_w_4.png)\n",
    "![nn_w_1.png](pics/nn_w_5.png)\n",
    "![nn_w_1.png](pics/nn_w_6.png)\n",
    "![nn_w_1.png](pics/nn_w_7.png)\n",
    "![nn_w_1.png](pics/nn_w_8.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "* You can use convolutional networks on top of characters (called learning from scratch)\n",
    "* It works best for large datasets where it beats classical approaches (like BOW)\n",
    "* Sometimes it even beats LSTM that works on word level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Quiz\n",
    "\n",
    "![quiz_1](pics/quiz_2_1.png)\n",
    "![quiz_1](pics/quiz_2_2.png)\n",
    "![quiz_1](pics/quiz_2_3.png)\n",
    "![quiz_1](pics/quiz_2_4.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
