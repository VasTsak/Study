{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Language modeling: it's all about counting! \n",
    "\n",
    "## 1. Count! N-gram language models\n",
    "\n",
    "![lm_1.png](pics/lm_1.png)\n",
    "![lm_1.png](pics/lm_2.png)\n",
    "![lm_1.png](pics/lm_3.png)\n",
    "![lm_1.png](pics/lm_4.png)\n",
    "![lm_1.png](pics/lm_5.png)\n",
    "![lm_1.png](pics/lm_6.png)\n",
    "![lm_1.png](pics/lm_7.png)\n",
    "What we get is for every sequence we get a sum probability of 1. But what we want is for all sequences to sum up to 1. How are we going to do that? \n",
    "\n",
    "We will add a fake token in the end, as we did in the beginning. And let us have this probability of the end token, given the last term. \n",
    "\n",
    "![lm_1.png](pics/lm_8.png)\n",
    "\n",
    "Why does that help?\n",
    "\n",
    "So imagine some generative process. Imagine how this model generates text words. Here is an example of how it generates different sequences:   \n",
    "\n",
    "![lm_1.png](pics/lm_9.png)\n",
    "\n",
    "This model has the option to terminate (one of cat's options is empty). That is not what it could do without the fake end token. So this is the place where things could go wrong if we did not add this fake token. \n",
    "![lm_1.png](pics/lm_10.png)\n",
    "\n",
    "So now we decide to pick something, we can split further and pick and split and so on. What is more important is that you can split also all the other areas in different parts. And then all of them will fit into this area. So all the sequences of different lengths altogether will give the probability mass equal to 1, which means that it is correctly a normalized probabilty. \n",
    "\n",
    "![lm_1.png](pics/lm_11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Perplexity: is our model surprised with a real text?\n",
    "\n",
    "![lm_12](pics/lm_12.png)\n",
    "The only difference with the previous model (bigram model) is that the history is longer. This is a notation of having n-1 sequence of words. \n",
    "\n",
    "![lm_13](pics/lm_13.png)\n",
    "\n",
    "If you take the derivatives of this likelihood, and also think about the constraints such as normalization and non-negativity of my parameters. And we would derive to exactly these formulas you see at the bottom of the slide above. Which is the **Likelihood maximization estimates**\n",
    "\n",
    "![lm_13](pics/lm_14.png)\n",
    "![lm_13](pics/lm_15.png)\n",
    "\n",
    "But how would you choose the best *n*?\n",
    "\n",
    "![lm_13](pics/lm_16.png)\n",
    "![lm_13](pics/lm_18.png)\n",
    "\n",
    "* We can compare it with an already developed program (Extrinsic evaluation)\n",
    "* We can use Intrinsic evaluation where we held out some data to compute perplexity later, but what is Perplexity? \n",
    "![lm_13](pics/lm_17.png)\n",
    "\n",
    "The lower perplexity the better! But why? Because the greater the likelihood is, the better. So the likelihood shows whether our model is surprised with our text or not, whether our model predicts the same test data that we have in real life. So perplexity has also this intuition. \n",
    "\n",
    "![lm_13](pics/lm_19.png)\n",
    "\n",
    "How can we fix that? There is a simple way to fix that. \n",
    "![lm_13](pics/lm_20.png)\n",
    "\n",
    "So let us say that we have some vocabulary, then we replace all out of vocabulary (OOV) words for a special UNK token. So then we compute our probabilities as usual for all vocabulary tokens and for the UNK token. We also see this UNK token in the training data. \n",
    "\n",
    "![lm_13](pics/lm_21.png)\n",
    "\n",
    "But this can happen with words from the vocabulary too. \n",
    "![lm_13](pics/lm_22.png)\n",
    "\n",
    "In this case we will use some smoothing techniques that you will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smoothing: what if we see new n-grams?\n",
    "\n",
    "The languange is really variate. It means that if we train a model on a train data, it is very likely that whether we apply this model to the test data, we will get some zeros. \n",
    "\n",
    "What can we do with those zeros? Could we substitute them with ones? Actually, we cannot do this and the reason is that in this way you **will not get a correct probability distribution**, so it will not be normalized into one. Instead we can do another simple thing. We can add one to all the counts even those that are not zeros. \n",
    "\n",
    "\n",
    "![lm_23](pics/lm_23.png)\n",
    "\n",
    "Let's see something more complicated. So we would like to use longer n-grams. It would be nice to use them but we might not have enough data to do this. So, the idea is, what if we try to use longer n-grams first and then, if we have not enough data to estimate the counts for them, we will become not that greedy and go for shorter n-grams? \n",
    "\n",
    "![lm_25.png](pics/lm_25.png)\n",
    "\n",
    "Why do we have some alphas$(\\alpha)$ there and also tilde$(\\tilde{p})$ near the B in the if branch? The reason is, that we still need to care about the probabilities. So those alpha constant is the discount that makes sure that the probability of all sequences will sum into one in our model. \n",
    "\n",
    "![lm_26.png](pics/lm_26.png)\n",
    "\n",
    "The same idea can be implemented in a different way. \n",
    "\n",
    "![lm_26.png](pics/lm_27.png)\n",
    "\n",
    "The goal of all of those methods is to pull the probability mass from frequent n-grams to infrequent n-grams. So, to what extent should we pull this probability mass? The answer for this question can be given by a nice experiment which was help in 1991. Let us stick to bigrams for now, and let us see that if you count the number of bigrams in your training data after that, you count the average number of the same bigrams in the test data. Those numbers are really correlated. So, you can see that if you're just substract 0.75 from your train data counts you will get very good estimates for your test data and this is a little bit magical property. \n",
    "\n",
    "![lm_26.png](pics/lm_28.png)\n",
    "\n",
    "So this is a property of the language that we can try to use. The way that we use it, is let us substract this D which is 0.75 is the extent of pull. Now, to give the probability to infrequent terms, we are using here unigram distributions. So in the right hand side, you will see some weight, that makes sure that normalization is fine and the unigram distribution. \n",
    "\n",
    "![lm_26.png](pics/lm_29.png)\n",
    "\n",
    "Now, can we do maybe something better than just a unigram distribution there? And this is the idea of Kneser-Ney smoothing. So, let us see this example. This is the malt or this is the Kong. So, the word Kong might be even more popular than the word malt but the thing is, that it *can only occur in a bigram Hong Kong*. So, the word Kong is **not very variative** in terms of different contexts that can go before it. And this why, we should **not prefer this word here to continue our phrase**. On the opposite, The word malt is not that popular but it can go nicely with different contexts. \n",
    "\n",
    "![lm_26.png](pics/lm_30.png)\n",
    "\n",
    "So, this idea is formalized with the formula in the top of this slide. Let us have the probability of the words proportional to how many different contexts can go just before the word. So, if you take your absolute discounting model and instead of unigram distribution have these nice distribution you will get Kneser-Ney smoothing. \n",
    "\n",
    "![lm_26.png](pics/lm_31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity computation\n",
    "\n",
    "Perplexity is a popular quality measure of language models. We can calculate it using the formula:\n",
    "\n",
    "$$\\mathcal{P} = p(w_{test})^{- \\frac{1}{N} }, \\ \\text{where} \\ p(w_{test}) = \\prod\\limits_{i=1}^{N+1} p(w_i|w_{i-n+1}^{i-1})$$\n",
    "\n",
    "Recall that all words of the corpus are concatenated and indexed in the range from 1 to $N$. So $N$ here is the length of the test corpus. Also recall that the tokens out of the range are fake start/end tokens to make the model correct.\n",
    "\n",
    "Check yourself: how many start and end tokens do we have in a trigram model?\n",
    "\n",
    "Now, if just one probability in the formula above is equal to zero, the whole probability of the test corpus is zero as well, so the perplexity is infinite. To avoid this problem, we can use different methods of smoothing. One of them is **Laplacian smoothing** (add-1 smoothing), which estimates probabilities with the formula:\n",
    "\n",
    "$$p(w_i|w^{i-1}_{i-n+1}) = \\frac{c(w^i_{i-n+1}) + 1}{c(w^{i-1}_{i-n+1}) + V}$$\n",
    "\n",
    "Note, that $V$ here is the number of possible continuations of the sequence $w_{i-n+1}^{i-1}$, so $V$ is the number of unique unigrams in the train corpus plus 1. Do you see why? Well, we include the fake end token to this number, because the model tries to predict it each time, just us as any other word. And we do not include the start tokens, because they serve only as a prefix for the first probabilities.\n",
    "\n",
    "Now, let’s review the following task together.\n",
    "\n",
    "Task:\n",
    "\n",
    "Apply add-one smoothing to the trigram language model trained on the sentence:\n",
    "\n",
    "“This is the cat that killed the rat that ate the malt that lay in the house that Jack built.”\n",
    "\n",
    "Find the perplexity of this smoothed model on the test sentence:\n",
    "\n",
    "“This is the house that Jack built.”\n",
    "\n",
    "Solution:\n",
    "\n",
    "We have n=3, so we will add two start tokens <s1>, <s2> and one end token <end>.\n",
    "\n",
    "Note, that we add (n-1) start tokens, since the start tokens are needed to condition the probability of the first word on them. The role of the end token is different and we always add just one end token. It's needed to be able to finish the sentence in the generative process at some point.\n",
    "\n",
    "So, what we have is:\n",
    "\n",
    "train: <s1> <s2> This is the cat that killed the rat that ate the malt that lay in the house that Jack built <end>\n",
    "\n",
    "test: <s1> <s2> This is the house that Jack built <end>\n",
    "\n",
    "Number of unique unigrams in train is 14, so V = 14 + 1 = 15.\n",
    "\n",
    "Number of words in the test sentence is 7, so N = 7.\n",
    "\n",
    "$$\\mathcal{P} = p(w_{test})^{- \\frac{1}{N} }, \\ \\text{where}\\ p(w_{test}) = \\prod\\limits_{i=1}^{8} p(w_i|w_{i-2} w_{i-1}) = \\prod\\limits_{i=1}^{8} \\frac{c(w_{i-2} w_{i-1} w_i) + 1}{c(w_{i-2} w_{i-1}) + 15}$$\n",
    "\n",
    "All right, now we need to compute 8 conditional probabilities. We can do it straightforwardly or notice a few things to make our life easier.\n",
    "\n",
    "First, note that all bigrams from the test sentence occur in the train sentence exactly once, which means we have (1 + 15) in all denominators.\n",
    "\n",
    "Also note, that \"is the house\" is the only trigram from the test sentence that is not present in the train sentence. The corresponding probability is p(house | is the) = (0 + 1) / (1 + 15) = 0.0625.\n",
    "\n",
    "All other trigrams from the test sentence occur in the train sentence exactly once. So their conditional probabilities will be equal to (1 + 1) / (1 + 15) = 0.125.\n",
    "\n",
    "In this way, perplexity is (0.0625 * 0.125 ^ 7) ^ (-1/7) = 11.89."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 0.23076923076923078\n",
      "is the house 0.15384615384615385\n",
      "the house that 0.23076923076923078\n",
      "house that Jack 0.23076923076923078\n",
      "that Jack built 0.23076923076923078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0201521395107633"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = \"This is the rat that ate the malt that lay in the house that Jack built\"\n",
    "train_unigrams = train_text.split()\n",
    "train_unique_unigrams = set(train_unigrams)\n",
    "n_train_unique_unigrams = len(train_unique_unigrams)\n",
    "\n",
    "test_text = \"This is the house that Jack built\"\n",
    "test_unigrams = test_text.split()\n",
    "test_unique_unigrams = set(test_unigrams)\n",
    "n_test_unique_unigrams = len(test_unique_unigrams)\n",
    "\n",
    "perplexity = 1\n",
    "for triagram_index_test in range(len(test_unigrams)-2):\n",
    "    trigram_occurence = 0\n",
    "    bigram_occurence = 0\n",
    "    for triagram_index_train in range(len(train_unigrams)-2):\n",
    "        if \" \".join(train_unigrams[triagram_index_train:triagram_index_train+3]) == \" \".join(test_unigrams[triagram_index_test:triagram_index_test+3]):\n",
    "            trigram_occurence += 1\n",
    "        if \" \".join(train_unigrams[triagram_index_train:triagram_index_train+2]) == \" \".join(test_unigrams[triagram_index_test:triagram_index_test+2]):\n",
    "            trigram_occurence += 1\n",
    "    perplexity_temp = (trigram_occurence + 1) / (bigram_occurence + n_train_unique_unigrams + 1)\n",
    "    print(\" \".join(test_unigrams[triagram_index_test:triagram_index_test+3]),str(perplexity_temp))\n",
    "    perplexity *= perplexity_temp\n",
    "\n",
    "perplexity ** (-1/len(test_unigrams)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test_unique_unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Language modelling quiz\n",
    "\n",
    "![lm_quiz_1](pics/lm_quiz_1.png)\n",
    "![lm_quiz_2](pics/lm_quiz_2.png)\n",
    "![lm_quiz_3](pics/lm_quiz_3.png)\n",
    "![lm_quiz_4](pics/lm_quiz_4.png)\n",
    "![lm_quiz_5](pics/lm_quiz_5.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
