{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "## What is PCA?\n",
    "Here you saw that PCA (or principal component analysis) is the first of the techniques you will see aimed at dimensionality reduction. This technique is about taking your full dataset and reducing it to only the parts that hold the most information.\n",
    "\n",
    "\n",
    "## How Well Will You Understand PCA After This Lesson?\n",
    "The goal is for everyone to leave this lesson with an understanding of:\n",
    "\n",
    "* How PCA is used in the world.\n",
    "* How to perform PCA in python.\n",
    "* A conceptual understanding of how the algorithm works.\n",
    "* How to interpret the results of PCA.\n",
    "* If you want to dive deeper into the mathematics, there will be additional links provided, but it will not be a main focus of this lesson.\n",
    "\n",
    "## PCA Lesson Topics\n",
    "\n",
    "There is a lot to cover with Principal Component Analysis (or PCA). However, you will gain a solid understanding of PCA by the end of this lesson, by applying this technique in a couple of scenarios using scikit-learn, and practicing interpreting the results.\n",
    "\n",
    "We will also cover conceptually how the algorithm works, and I will provide links to explore what is happening mathematically in case you want to dive in deeper! Here is an outline of what you can expect in this lesson.\n",
    "\n",
    "### 1. Dimensionality Reduction through Feature Selection and Feature Extraction\n",
    "With large datasets we often suffer with what is known as the \"curse of dimensionality,\" and need to reduce the number of features to effectively develop a model. Feature Selection and Feature Extraction are two general approaches for reducing dimensionality.\n",
    "\n",
    "### 2. Feature Extraction using PCA\n",
    "Principal Component Analysis is a common method for extracting new \"latent features\" from our dataset, based on existing features.\n",
    "\n",
    "### 3. Fitting PCA\n",
    "In this part of the lesson, you will use PCA in scikit-learn to reduce the dimensionality of images of handwritten digits.\n",
    "\n",
    "### 4. Interpreting Results\n",
    "Once you are able to use PCA on a dataset, it is essential that you know how to interpret the results you get back. There are two main parts to interpreting your results - the principal components themselves and the variability of the original data captured by those components. You will get familiar with both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent features\n",
    "\n",
    "Latent features are features that aren't explicitly in your dataset.\n",
    "![pca_1.png](pics/pca_1.png)\n",
    "In this example, you saw that the following features are all related to the latent feature **home size**:\n",
    "\n",
    "* lot size\n",
    "* number of rooms\n",
    "* floor plan size\n",
    "* size of garage\n",
    "* number of bedrooms\n",
    "* number of bathrooms\n",
    "\n",
    "Similarly, the following features could be reduced to a single latent feature of **home neighborhood**:\n",
    "\n",
    "* local crime rate\n",
    "* number of schools in five miles\n",
    "* property tax rate\n",
    "* local median income\n",
    "* average air quality index\n",
    "* distance to highway\n",
    "\n",
    "So even if our original dataset has the 12 features listed, we might be able to reduce this to only 2 latent features relating to the home size and home neighborhood.\n",
    "\n",
    "![pca_1.png](pics/pca_2.png)\n",
    "\n",
    "\n",
    "## Reducing the Number of Features - Dimensionality Reduction\n",
    "\n",
    "Our real estate example is great to help develop an understanding of feature reduction and latent features. But we have a smallish number of features in this example, so it's not clear why it's so necessary to reduce the number of features. And in this case it wouldn't actually be required - we could handle all six original features to create a model.\n",
    "\n",
    "But the \"curse of dimensionality\" becomes more clear when we're grappling with large real-world datasets that might involve hundreds or thousands of features, and to effectively develop a model really requires us to reduce our number of dimensions.\n",
    "\n",
    "### Two Approaches : Feature Selection and Feature Extraction\n",
    "#### Feature Selection\n",
    "Feature Selection involves finding a subset of the original features of your data that you determine are most relevant and useful. In the example image below, taken from the video, notice that \"floor plan size\" and \"local crime rate\" are features that we have selected as a subset of the original data.\n",
    "\n",
    "![pca_1.png](pics/pca_3.png)\n",
    "\n",
    "* **Filter methods** - Filtering approaches use a ranking or sorting algorithm to filter out those features that have less usefulness. Filter methods are based on discerning some inherent correlations among the feature data in unsupervised learning, or on correlations with the output variable in supervised settings. Filter methods are usually applied as a preprocessing step. Common tools for determining correlations in filter methods include: *Pearson's Correlation, Linear Discriminant Analysis (LDA), and Analysis of Variance (ANOVA)*.\n",
    "* **Wrapper methods** - Wrapper approaches generally select features by directly testing their impact on the performance of a model. The idea is to \"wrap\" this procedure around your algorithm, repeatedly calling the algorithm using different subsets of features, and measuring the performance of each model. *Cross-validation* is used across these multiple tests. The features that produce the best models are selected. Clearly this is a computationally expensive approach for finding the best performing subset of features, since they have to make a number of calls to the learning algorithm. Common examples of wrapper methods are: *Forward Search, Backward Search, and Recursive Feature Elimination*\n",
    ".\n",
    "Scikit-learn has a feature selection module that offers a variety of methods to improve model accuracy scores or to boost their performance on very high-dimensional datasets.\n",
    "\n",
    "#### Feature Extraction\n",
    "Feature Extraction involves extracting, or constructing, new features called latent features. In the example image below, taken from the video, \"Size Feature\" and \"Neighborhood Quality Feature\" are new latent features, extracted from the original input data.\n",
    "\n",
    "![pca_1.png](pics/pca_4.png)\n",
    "\n",
    "##### Methods of Feature Extraction\n",
    "Constructing latent features is exactly the goal of **Principal Component Analysis** (PCA), which we'll explore throughout the rest of this lesson.\n",
    "\n",
    "Other methods for accomplishing Feature Extraction include **Independent Component Analysis** (ICA) and **Random Projection**, which we will study in the following lesson.\n",
    "\n",
    "**Further Exploration**\n",
    "If you're interested in deeper study of these topics, here are a couple of helpful blog posts and a research paper:\n",
    "\n",
    "* [https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)\n",
    "* [https://elitedatascience.com/dimensionality-reduction-algorithms](https://elitedatascience.com/dimensionality-reduction-algorithms)\n",
    "* [http://www.ai.mit.edu/projects/jmlr/papers/volume3/guyon03a/source/old/guyon03a.pdf](http://www.ai.mit.edu/projects/jmlr/papers/volume3/guyon03a/source/old/guyon03a.pdf)\n",
    "\n",
    "\n",
    "## Principal Components\n",
    "\n",
    "![pca_1.png](pics/pca_5.png)\n",
    "\n",
    "![pca_1.png](pics/pca_6.png)\n",
    "\n",
    "An advantage of **Feature Extraction** over **Feature Selection** is that the latent features can be constructed to incorporate data from multiple features, and thus retain more information present in the various original inputs, than just losing that information by dropping many original inputs.\n",
    "\n",
    "Principal components are **linear combinations** of the original features in a dataset that aim to retain the most information in the original data.\n",
    "\n",
    "You can think of a principal component in the same way that you think about a latent feature.\n",
    "\n",
    "The general approach to this problem of high-dimensional datasets is to search for a projection of the data onto a smaller number of features which preserves the information as much as possible.\n",
    "\n",
    "## PCA Properties\n",
    "There are two main properties of principal components:\n",
    "\n",
    "They retain the most amount of information in the dataset. You can see that retaining the most information in the dataset meant finding a line that reduced the distances of the points to the component across all the points (same as in regression!).\n",
    "\n",
    "![pca_1.png](pics/pca_7.png)\n",
    "\n",
    "The created components are orthogonal to one another. So far we have been mostly focused on what the first component of a dataset would look like. However, when there are many components, the additional components will all be orthogonal to one another. Depending on how the components are used, there are benefits to having orthogonal components. In regression, we often would like independent features, so using the components in regression now guarantees this.\n",
    "\n",
    "![pca_1.png](pics/pca_8.png)\n",
    "\n",
    "### Quiz\n",
    "\n",
    "![pca_1.png](pics/pca_q_1.png)\n",
    "![pca_1.png](pics/pca_q_2.png)\n",
    "![pca_1.png](pics/pca_q_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Eigenvalues and Eigenvectors?\n",
    "The mathematics of PCA isn't really necessary for PCA to be useful. However, it can be useful to fully understand the mathematics of a technique to understand how it might be extended to new cases. For this reason, the page has a few additional references which go more into the mathematics of PCA.\n",
    "\n",
    "A simple introduction of what PCA is aimed to accomplish is provided here in a simple example.\n",
    "\n",
    "A nice visual, and mathematical, illustration of PCA is provided in this video by 3 blue 1 brown.\n",
    "\n",
    "https://www.youtube.com/watch?v=PFDu9oVAE-g\n",
    "\n",
    "If you dive into the literature surrounding PCA, you will without a doubt run into the language of eigenvalues and eigenvectors. These are just the math-y words for things you have already encountered in this lesson.\n",
    "\n",
    "An eigenvalue is the same as the amount of variability captured by a principal component, and an eigenvector is the principal component itself. To see more on these ideas, take a look at the following three links below:\n",
    "\n",
    "[A great introduction into the mathematics of principal components analysis.](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\n",
    "\n",
    "[An example of using PCA in python by one of my favorite data scientists.](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "\n",
    "[An example of PCA from the scikit learn documentation.](http://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py)\n",
    "\n",
    "\n",
    "## When to use PCA? \n",
    "\n",
    "1. Every time you want to reduce the dimensionality of your data \n",
    "3. Find latent features that might capture a number of other features into a single factor.\n",
    "\n",
    "![pca](pics/pca_11.png)\n",
    "\n",
    "![pca](pics/pca_12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### 1. Two Methods for Dimensionality Reduction\n",
    "\n",
    "You learned that Feature Selection and Feature Extraction are two general approaches for reducing the number of features in your data. Feature Selection processes result in a subset of the most significant original features in the data, while Feature Extraction methods like PCA construct new latent features that well represent the original data.\n",
    "\n",
    "### 2. Dimensionality Reduction and Principal Components\n",
    "You learned that Principal Component Analysis (PCA) is a technique that is used to reduce the dimensionality of your dataset. The reduced features are called principal components, or latent features. These principal components are simply a linear combination of the original features in your dataset.\n",
    "\n",
    "You learned that these components have two major properties:\n",
    "\n",
    "1. They aim to capture the most amount of variability in the original dataset.\n",
    "1. They are orthogonal to (independent of) one another.\n",
    "\n",
    "### 3. Fitting PCA\n",
    "Once you got the gist of what PCA was doing, we used it on handwritten digits within scikit-learn.\n",
    "\n",
    "We did this all within a function called `do_pca`, which returned the PCA model, as well as the reduced feature matrix. You simply passed in the number of features you wanted back, as well as the original dataset.\n",
    "\n",
    "### 4. Interpreting Results\n",
    "You then saw there are two major parts to interpreting the PCA results:\n",
    "\n",
    "1. The variance explained by each component. You were able to visualize this with scree plots to understand how many components you might keep based on how much information was being retained.\n",
    "2. The principal components themselves, which gave us an idea of which original features were most related to why a component was able to explain certain aspects about the original datasets.\n",
    "\n",
    "### 5. Mini-project\n",
    "Finally, you applied PCA to a dataset on vehicle information. You gained valuable experience using scikit-learn, as well as interpreting the results of PCA.\n",
    "\n",
    "With mastery of these skills, you are now ready to use PCA for any task in which you feel it may be useful. If you have a large amount of data, and are feeling afflicted by the curse of dimensionality, you want to reduce your data to a smaller number of latent features, and you know just the way to do it!\n",
    "\n",
    "### 6. Do you think you understand PCA well enough yet to explain it in a way that would make sense to your grandmother?\n",
    "Here is an interesting StackExchange post that does just that, and with animated graphics! https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
