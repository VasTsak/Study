{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary line\n",
    "\n",
    "![lb1](pics/boundary_line_1.png)\n",
    "\n",
    "![lb2](pics/line_boundary_2.png)\n",
    "\n",
    "## Higher Dimensions\n",
    "\n",
    "![h_d_1](pics/higher_dim.png)\n",
    "\n",
    "### QUIZ QUESTION\n",
    "\n",
    "![h_d_q](pics/h_d_quiz.png)\n",
    "\n",
    "## Perceptrons as Logical Operators\n",
    "\n",
    "In this lesson, we'll see one of the many great applications of perceptrons. As logical operators! You'll have the chance to create the perceptrons for the most common of these, the **AND, OR**, and **NOT** operators. And then, we'll see what to do about the elusive **XOR** operator. Let's dive in!\n",
    "\n",
    "### AND Perceptron\n",
    "\n",
    "![and-quiz.png](pics/and-quiz.png)\n",
    "\n",
    "#### What are the weights and bias for the AND perceptron?\n",
    "\n",
    "Set the weights (`weight1`, `weight2`) and bias (`bias`) to values that will correctly determine the AND operation as shown above.\n",
    "More than one set of values will work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                  -1.0                    0          Yes\n",
      "       0          1                  -0.5                    0          Yes\n",
      "       1          0                  -0.5                    0          Yes\n",
      "       1          1                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.5\n",
    "weight2 = 0.5\n",
    "bias = -1.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Perceptron\n",
    "\n",
    "![or-quiz.png](pics/or-quiz.png)\n",
    "\n",
    "The OR perceptron is very similar to an AND perceptron. In the image below, the OR perceptron has the same line as the AND perceptron, except the line is shifted down. What can you do to the weights and/or bias to achieve this? Use the following AND perceptron to create an OR Perceptron.\n",
    "\n",
    "![and-to-or.png](pics/and-to-or.png)\n",
    "\n",
    "![and_or.png](pics/and_or.png)\n",
    "\n",
    "### NOT Perceptron\n",
    "\n",
    "Unlike the other perceptrons we looked at, the NOT operation only cares about one input. The operation returns a `0` if the input is `1` and a `1` if it's a `0`. The other inputs to the perceptron are ignored.\n",
    "\n",
    "In this quiz, you'll set the weights (`weight1`, `weight2`) and bias `bias` to the values that calculate the NOT operation on the second input and ignores the first input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                   0.0                    1          Yes\n",
      "       0          1                  -1.0                    0          Yes\n",
      "       1          0                   0.0                    1          Yes\n",
      "       1          1                  -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1.0\n",
    "bias = 0.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Perceptron\n",
    "\n",
    "![xor.png](pics/xor.png)\n",
    "\n",
    "Now, let's build a multi-layer perceptron from the AND, NOT, and OR perceptrons to create XOR logic!\n",
    "\n",
    "The neural network below contains 3 perceptrons, A, B, and C. The last one (AND) has been given for you. The input to the neural network is from the first node. The output comes out of the last node.\n",
    "\n",
    "The multi-layer perceptron below calculates XOR. Each perceptron is a logic operation of AND, OR, and NOT. However, the perceptrons A, B, and C don't indicate their operation. In the following quiz, set the correct operations for the perceptrons to calculate XOR.\n",
    "\n",
    "![xor_nn.png](pics/xor_nn.png)\n",
    "\n",
    "## Perceptron Trick\n",
    "\n",
    "In the last section you used your logic and your mathematical knowledge to create perceptrons for some of the most common logical operators. In real life, though, we can't be building these perceptrons ourselves. The idea is that we give them the result, and they build themselves. For this, here's a pretty neat trick that will help us.\n",
    "\n",
    "\n",
    "![perceptronquiz.png](pics/perceptronquiz.png)\n",
    "\n",
    "**Solution**: Closer\n",
    "\n",
    "## Time for some math!\n",
    "\n",
    "Now that we've learned that the points that are misclassified, want the line to move closer to them, let's do some math. The following video shows a mathematical trick that modifies the equation of the line, so that it comes closer to a particular point.\n",
    "\n",
    "![perceptron_trick_1.png](pics/perceptron_trick_1.png)\n",
    "\n",
    "\n",
    "### Quiz\n",
    "\n",
    "For the second example, where the line is described by $3x1+ 4x2 - 10 = 0$, if the learning rate was set to $0.1$, how many times would you have to apply the perceptron trick to move the line to a position where the blue point, at $(1, 1)$, is correctly classified?\n",
    "\n",
    "**Solution:** 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm\n",
    "\n",
    "And now, with the perceptron trick in our hands, we can fully develop the perceptron algorithm!\n",
    "\n",
    "There's a small error in the above video in that $W_i$ should be updated to $W_i = W_i + \\alpha x_i$ (plus or minus depending on the situation).\n",
    "\n",
    "### Coding the Perceptron Algorithm\n",
    "\n",
    "Implement the perceptron algorithm.\n",
    "\n",
    "![points.png](pics/points.png)\n",
    "\n",
    "Recall that the perceptron step works as follows. For a point with coordinates $(p,q)$, label $y$, and prediction given by the equation $\\hat{y} = step(w_1x_1 + w_2x_2 + b)$:\n",
    "\n",
    "If the point is correctly classified, do nothing.\n",
    "If the point is classified positive, but it has a negative label, subtract $\\alpha$ $p$, $\\alpha$ $q$, and $\\alpha$ from $w_1$, $w_2$, and $b$ respectively.\n",
    "If the point is classified negative, but it has a positive label, add $\\alpha p$, $\\alpha q$, and $\\alpha$ to $w_1, w_2$, and $b$ respectively.\n",
    "\n",
    "Feel free to play with the parameters of the algorithm (number of epochs, learning rate, and even the randomizing of the initial parameters) to see how your initial conditions can affect the solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    for i in range(len(X)):\n",
    "        if prediction(X[i], W, b) > y[i]:\n",
    "            W[0] -= learn_rate * X[i][0]\n",
    "            W[1] -= learn_rate * X[i][1]\n",
    "            b -= learn_rate \n",
    "        if prediction(X[i], W, b) < y[i]:\n",
    "            W[0] -= learn_rate * X[i][0]\n",
    "            W[1] -= learn_rate * X[i][1]\n",
    "            b += learn_rate \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function\n",
    "\n",
    "An error function is a function that tell us how far we are from the solution. \n",
    "\n",
    "### Log-loss Error Function\n",
    "\n",
    "![log-loss-error-function.png](pics/log-loss-error-function.png)\n",
    "\n",
    "\n",
    "### Discrete vs Continuous Predictions\n",
    "\n",
    "\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_1.png)\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_2.png)\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_3.png)\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_4.png)\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_5.png)\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_6.png)\n",
    "![discrete_continuous_1.png](pics/discrete_continuous_7.png)\n",
    "\n",
    "## The Softmax Function\n",
    "\n",
    "Now you'll learn about the softmax function, which is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.\n",
    "\n",
    "Let´s assume you want to predict if you receive a gift or not. \n",
    "\n",
    "![softmax_1.png](pics/softmax_1.png)\n",
    "\n",
    "Let us consider now a problem where we want our model to tell us if the animal is a duck, biever or a walrus. \n",
    "\n",
    "![softmax_1.png](pics/softmax_2.png)\n",
    "\n",
    "\n",
    "![softmax_1.png](pics/softmax_quiz_1.png)\n",
    "\n",
    "**Solution:** *exp*\n",
    "\n",
    "![softmax_1.png](pics/softmax_3.png)\n",
    "\n",
    "![softmax_1.png](pics/softmax_4.png)\n",
    "\n",
    "**Solution:** *yes*\n",
    "\n",
    "### Coding Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    sf = []\n",
    "    for l in L:\n",
    "        sfDum = np.exp(l)/sumExpL\n",
    "        sf.append(sfDum)\n",
    "    return sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood \n",
    "\n",
    "Probability will be one of our best friends as we go through Deep Learning. In this lesson, we'll see how we can use probability to evaluate (and improve!) our models.\n",
    "\n",
    "In this methodology we pick the models that give the highest probability at the correct predicted value of label. \n",
    "\n",
    "![maximum_likelihood_1.png](pics/maximum_likelihood_1.png) \n",
    "\n",
    "What we mean by this is that if the model is given by these probability spaces, then the probability that the points are of these colours is 0.0084.\n",
    "\n",
    "Let´s do the same for another model. \n",
    "\n",
    "![maximum_likelihood_1.png](pics/maximum_likelihood_2.png) \n",
    "\n",
    "\n",
    "We can see that the second model is much better, so we should be going for that. \n",
    "\n",
    "### Maximizing Probabilities\n",
    "\n",
    "In this lesson and quiz, we will learn how to maximize a probability, using some math. Nothing more than high school math, so get ready for a trip down memory lane!\n",
    "\n",
    "As we saw before, our goal is to minimize the loss function. Could it be that the minimization of the loss function and maximization of probability be connected? \n",
    "\n",
    "## Cross-Entropy\n",
    "\n",
    "So all we need is to maximize the total probability, is to find the model with the maximum probability. But then we need to do the **product** of all the probabilities, and products are hard - but why?\n",
    "1. When we have thousands of probabilities, then the maximum probability is tiny. \n",
    "2. when single a probability change, the product will change drastically.\n",
    "\n",
    "In summary, we really want to stay away from products. Let´s do sum! How can we do it? With **log**!\n",
    "\n",
    "![cross_entropy_1.png](pics/cross_entropy_1.png)\n",
    "\n",
    "So we're getting somewhere, there's definitely a connection between probabilities and error functions, and it's called **Cross-Entropy**. This concept is tremendously popular in many fields, including Machine Learning. Let's dive more into the formula, and actually code it!\n",
    "\n",
    "Essentially what Cross-Entropy says is that given some events with their corresponding probabilities, if their probability of occuring is high and they occur then we have a small Cross-Entropy, if they have a small probability of occuring but they occur then we have a big Cross-Entropy. \n",
    "\n",
    "$$-\\sum_{i = 1}^{m}y_i ln(p_i) + (1 - y_i) ln(1 - p_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    pr = [-y * np.log(p) - (1 - y) * np.log(1 - p) for y, p in zip(Y, P)]\n",
    "    ce = sum(pr)\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class cross Entropy \n",
    "\n",
    "![multi_class_entropy.png](pics/multi_class_entropy.png)\n",
    "\n",
    "$$\\text{Corss-Entropy = } -\\sum_{i = 1}^{n}\\sum_{j = 1}^{m}y_{ij}ln(p_{ij})$$\n",
    "\n",
    "where $m$ is the number of classes.\n",
    "\n",
    "![multi-entropy-quiz.png](pics/multi-entropy-quiz.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now, we're finally ready for one of the most popular and useful algorithms in Machine Learning, and the building block of all that constitutes Deep Learning. The Logistic Regression Algorithm. And it basically goes like this:\n",
    "\n",
    "* Take your data\n",
    "* Pick a random model\n",
    "* Calculate the error\n",
    "* Minimize the error, and obtain a better model\n",
    "* Enjoy!\n",
    "\n",
    "### Calculating the Error Function\n",
    "\n",
    "We will use the Cross-entropy as seen before, but we will take the average of it, not the sum of it. \n",
    "\n",
    "![error-funciton.png](pics/error-funciton.png)\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "We learned that in order to minimize the error function, we need to take some derivatives. So let's get our hands dirty and actually compute the derivative of the error function. The first thing to notice is that the sigmoid function has a really nice derivative. Namely,\n",
    "\n",
    "![codecogseqn-49.gif](pics/codecogseqn-49.gif)\n",
    "\n",
    "And now, let's recall that if we have mm points labelled $x^{(1)}$, $x^{(2)}$, $\\ldots, x^{(m)}$, the error formula is:\n",
    "\n",
    "$E = -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i \\ln(\\hat{y_i}) + (1-y_i) \\ln (1-\\hat{y_i}) \\right)$\n",
    "\n",
    "where the prediction is given by $\\hat{y_i} = \\sigma(Wx^{(i)} + b)$. \n",
    "\n",
    "Our goal is to calculate the gradient of $E,$ at a point $x = (x_1, \\ldots, x_n)$, given by the partial derivatives\n",
    "\n",
    "$\\nabla E =\\left(\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E \\right)$\n",
    "\n",
    "To simplify our calculations, we'll actually think of the error that each point produces, and calculate the derivative of this error. The total error, then, is the average of the errors at all the points. The error produced by each point is, simply,\n",
    "\n",
    "$$E = - y \\ln(\\hat{y}) - (1-y) \\ln (1-\\hat{y})$$\n",
    "\n",
    "In order to calculate the derivative of this error with respect to the weights, we'll first calculate $\\frac{\\partial}{\\partial w_j} \\hat{y}$. Recall that $\\hat{y} = \\sigma(Wx+b)$, so:\n",
    "\n",
    "![codecogseqn-43.gif](pics/codecogseqn-43.gif)\n",
    "\n",
    "The last equality is because the only term in the sum which is not a constant with respect to $w_j$, which clearly has derivative $x_j$.\n",
    "\n",
    "Now, we can go ahead and calculate the derivative of the error $E$ at a point $x$, with respect to the weight $w_j$.\n",
    "\n",
    "![codecogseqn-60-2.png](pics/codecogseqn-60-2.png)\n",
    "\n",
    "A similar calculation will show us that\n",
    "\n",
    "![codecogseqn-58.gif](pics/codecogseqn-58.gif)\n",
    "\n",
    "This actually tells us something very important. For a point with coordinates $(x_1, \\ldots, x_n)$, label $y$, and prediction $\\hat{y},$ the gradient of the error function at that point is $\\left(-(y - \\hat{y})x_1, \\cdots, -(y - \\hat{y})x_n, -(y - \\hat{y}) \\right)$. In summary, the gradient is\n",
    "\n",
    "$\\nabla E = -(y - \\hat{y}) (x_1, \\ldots, x_n, 1)$.\n",
    "\n",
    "If you think about it, this is fascinating. **The gradient is actually a scalar times the coordinates of the point!** And what is the scalar? Nothing less than a **multiple of the difference between the label and the prediction**. What significance does this have?\n",
    "\n",
    "![gd_quiz_1.png](pics/gd_quiz_1.png)\n",
    "\n",
    "So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n",
    "\n",
    "If this sounds anything like the perceptron algorithm, this is no coincidence! We'll see it in a bit.\n",
    "\n",
    "### Gradient Descent Step\n",
    "Therefore, since the gradient descent step simply consists in **subtracting a multiple of the gradient of the error function at every point**, then this updates the weights in the following way:\n",
    "\n",
    "$$w_i' \\leftarrow w_i -\\alpha [-(y - \\hat{y}) x_i],$$ which is equivalent to\n",
    "\n",
    "\n",
    "$$w_i' \\leftarrow w_i + \\alpha (y - \\hat{y}) x_i.$$\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "$$b' \\leftarrow b + \\alpha (y - \\hat{y}),$$\n",
    "\n",
    "*Note*: Since we've taken the average of the errors, the term we are adding should be $\\frac{1}{m} \\cdot \\alpha $ instead of $\\alpha$, but as $\\alpha$ is a constant, then in order to simplify calculations, we'll just take $\\frac{1}{m} \\cdot \\alpha$ to be our learning rate, and abuse the notation by just calling it $\\alpha.$\n",
    "\n",
    "![Gradient_descent_algorithm.png](pics/Gradient_descent_algorithm.png)\n",
    "\n",
    "\n",
    "### Gradient descent vs Perceptron\n",
    "\n",
    "![gradient_descent_vs_perceptron.png](pics/gradient_descent_vs_perceptron.png) \n",
    "\n",
    "The difference between perceptron and gradient descent algorithm is that perceptron does nothing if the point was classified correctly while gradient descent algorithm sends the boundary further away to minimiize the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Models\n",
    "\n",
    "![nl_1.png](pics/nl_1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Ok, so we're ready to put these building blocks together, and build great Neural Networks! (Or Multi-Layer Perceptrons, however you prefer to call them.)\n",
    "\n",
    "![nn_1.png](pics/nn_1.png)\n",
    "\n",
    "![nn_2.png](pics/nn_2.png)\n",
    "\n",
    "![nn_3.png](pics/nn_3.png)\n",
    "\n",
    "![nn_4.png](pics/nn_4.png)\n",
    "\n",
    "![nn_4.png](pics/nn_5.png)\n",
    "\n",
    "\n",
    "#### QUESTION 1 OF 2\n",
    "\n",
    "Let's define the combination of two new perceptrons as w1\\*0.4 + w2\\*0.6 + b. Which of the following values for the weights and the bias would result in the final probability of the point to be 0.88?\n",
    "\n",
    "Solution: w1: 3, w2: 5, b:-2.2\n",
    "\n",
    "### Multiple layers\n",
    "\n",
    "Now, not all neural networks look like the one above. They can be way more complicated! In particular, we can do the following things:\n",
    "\n",
    "* Add more nodes to the input, hidden, and output layers.\n",
    "* Add more layers.\n",
    "We'll see the effects of these changes below.\n",
    "\n",
    "![nn_4.png](pics/nn_6.png)\n",
    "![nn_4.png](pics/nn_7.png)\n",
    "![nn_4.png](pics/nn_8.png)\n",
    "![nn_4.png](pics/nn_9.png)\n",
    "![nn_4.png](pics/nn_10.png)\n",
    "\n",
    "### Multi-Class Classification\n",
    "\n",
    "And here we elaborate a bit more into what can be done if our neural network needs to model data with more than one output.\n",
    "\n",
    "![nn_4.png](pics/nn_11.png)\n",
    "![nn_4.png](pics/nn_12.png)\n",
    "\n",
    "#### QUESTION 2 OF 2\n",
    "\n",
    "How many nodes in the output layer would you require if you were trying to classify all the letters in the English alphabet?\n",
    "\n",
    "Solution: 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward\n",
    "\n",
    "Feedforward is the process neural networks use to turn the input into an output. Let's study it more carefully, before we dive into how to train the networks.\n",
    "\n",
    "![nn_13](pics/nn_13.png)\n",
    "![nn_13](pics/nn_14.png)\n",
    "\n",
    "![nn_13](pics/nn_15.png)\n",
    "\n",
    "### Error Function\n",
    "\n",
    "Just as before, neural networks will produce an error function, which at the end, is what we'll be minimizing. The following video shows the error function for a neural network.\n",
    "\n",
    "![nn_13](pics/nn_16.png)\n",
    "![nn_13](pics/nn_17.png)\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Now, we're ready to get our hands into training a neural network. For this, we'll use the method known as **backpropagation**. In a nutshell, backpropagation will consist of:\n",
    "\n",
    "* Doing a feedforward operation.\n",
    "* Comparing the output of the model with the desired output.\n",
    "* Calculating the error.\n",
    "* Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "* Use this to update the weights, and get a better model.\n",
    "* Continue this until we have a model that is good.\n",
    "\n",
    "![nn_13](pics/nn_18.png)\n",
    "![nn_13](pics/nn_19.png)\n",
    "![nn_13](pics/nn_20.png)\n",
    "![nn_13](pics/nn_21.png)\n",
    "\n",
    "### Backpropagation \n",
    "\n",
    "Feel free to tune out, since this part gets handled by Keras pretty well. If you'd like to go start training networks right away, go to the next section. But if you enjoy calculating lots of derivatives, let's dive in!\n",
    "\n",
    "![nn_13](pics/nn_22.png)\n",
    "![nn_13](pics/nn_23.png)\n",
    "![nn_13](pics/nn_24.png)\n",
    "\n",
    "\n",
    "#### Chain Rule\n",
    "![nn_13](pics/nn_25.png)\n",
    "\n",
    "![nn_13](pics/nn_26.png)\n",
    "![nn_13](pics/nn_27.png)\n",
    "![nn_13](pics/nn_28.png)\n",
    "\n",
    "### Calculation of the derivative of the sigmoid function\n",
    "\n",
    "Recall that the sigmoid function has a beautiful derivative, which we can see in the following calculation. This will make our backpropagation step much cleaner.\n",
    "\n",
    "![sigmoid-derivative.gif](pics/sigmoid-derivative.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
